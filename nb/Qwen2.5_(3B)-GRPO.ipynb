{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EnZQsVsB2oP5"
      },
      "source": [
        "To run this, press \"*Runtime*\" and press \"*Run all*\" on a **free** Tesla T4 Google Colab instance!\n",
        "<div class=\"align-center\">\n",
        "<a href=\"https://unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n",
        "<a href=\"https://discord.gg/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord button.png\" width=\"145\"></a>\n",
        "<a href=\"https://docs.unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/blob/main/images/documentation%20green%20button.png?raw=true\" width=\"125\"></a></a> Join Discord if you need help + ‚≠ê <i>Star us on <a href=\"https://github.com/unslothai/unsloth\">Github</a> </i> ‚≠ê\n",
        "</div>\n",
        "\n",
        "To install Unsloth on your own computer, follow the installation instructions on our Github page [here](https://docs.unsloth.ai/get-started/installing-+-updating).\n",
        "\n",
        "You will learn how to do [data prep](#Data), how to [train](#Train), how to [run the model](#Inference), & [how to save it](#Save)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vY1DypOT2oP8"
      },
      "source": [
        "### News"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cfn3ib72oP8"
      },
      "source": [
        "**Read our [blog post](https://unsloth.ai/blog/r1-reasoning) for guidance on how to train reasoning models.**\n",
        "\n",
        "Visit our docs for all our [model uploads](https://docs.unsloth.ai/get-started/all-our-models) and [notebooks](https://docs.unsloth.ai/get-started/unsloth-notebooks).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ITV7lM3k2oP8"
      },
      "source": [
        "### Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "uUVpdB2o2oP8"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# Skip restarting message in Colab\n",
        "import sys; modules = list(sys.modules.keys())\n",
        "for x in modules: sys.modules.pop(x) if \"PIL\" in x or \"google\" in x else None\n",
        "\n",
        "!pip install unsloth vllm\n",
        "!pip install --upgrade pillow\n",
        "# If you are running this notebook on local, you need to install `diffusers` too\n",
        "# !pip install diffusers\n",
        "# Temporarily install a specific TRL nightly version\n",
        "!pip install git+https://github.com/huggingface/trl.git@e95f9fb74a3c3647b86f251b7e230ec51c64b72b"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IKG9LqHN2oP9"
      },
      "source": [
        "### Unsloth"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y1cRtXR2yknS"
      },
      "source": [
        "Use `PatchFastRL` before all functions to patch GRPO and other RL algorithms!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "59DIs5BMcvjN",
        "outputId": "e294858c-ed9a-445e-c290-3fa34f2f7dcd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "ü¶• Unsloth Zoo will now patch everything to make training faster!\n",
            "INFO 02-12 19:09:55 __init__.py:190] Automatically detected platform cuda.\n"
          ]
        }
      ],
      "source": [
        "from unsloth import FastLanguageModel, PatchFastRL\n",
        "PatchFastRL(\"GRPO\", FastLanguageModel)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Joje4qPsyxM9"
      },
      "source": [
        "Load up `Qwen 2.5 3B Instruct`, and set parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 594,
          "referenced_widgets": [
            "502a4d1f11144c06b514627a9837c098",
            "4d6b2701a4904a3caefb5a61de8604fe",
            "25b02faaa37543fab9cd5cf4d9ded89c",
            "9de1ced1719d4e93bd970f4734b5b4ef",
            "b5a517d5225a4d5a968b7dac1f9d1c53",
            "739d502c79404f438b7e5cf0d69ed710",
            "7e85a2be6ec84cfd990bef6f2c7fc0eb",
            "5d0e808b54324eedb7c55f32b6551647",
            "30c15883925d4a8dbd504abf6b7dff0f",
            "7984a46caf4d4fd7880132d386ffdfee",
            "ccfd6b0f8d034af9b4b82dd35db23aa6",
            "bc7abffebcde4fb3ba06ae2310bf2896",
            "32992b80b6074eb880f08eddbc2cbebc",
            "cf3ca4b73f444f5183030d2696fee992",
            "bca0ce2597954f5aa1520d03371fc52b",
            "a6f550d797b8403880b725d1f95464f5",
            "3787c8be64414d08b1b3e595440b9ee2",
            "8f31da0f871c48c785224f74162d1f59",
            "cb58b76dc83a42658a8bbc4dd6ed5bb2",
            "3663dc1e8e454d2ba17fc90e0d809d67",
            "56d47b464e27400a9ae384255a96ed76",
            "359e37cc82344a10824d1a7c24d819e9"
          ]
        },
        "id": "DkIvEkIIkEyB",
        "outputId": "e705cc71-8473-439d-b59e-039dd89ef083"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==((====))==  Unsloth 2025.2.5: Fast Llama patching. Transformers: 4.48.2.\n",
            "   \\\\   /|    GPU: NVIDIA A100-SXM4-40GB. Max memory: 39.557 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.5.1+cu124. CUDA: 8.0. CUDA Toolkit: 12.4. Triton: 3.1.0\n",
            "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post3. FA2 = False]\n",
            " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
            "Unsloth: vLLM loading unsloth/meta-llama-3.1-8b-instruct-bnb-4bit with actual GPU utilization = 49.48%\n",
            "Unsloth: Your GPU has CUDA compute capability 8.0 with VRAM = 39.56 GB.\n",
            "Unsloth: Using conservativeness = 1.0. Chunked prefill tokens = 32000. Num Sequences = 256.\n",
            "Unsloth: vLLM's KV Cache can use up to 13.24 GB. Also swap space = 6 GB.\n",
            "INFO 02-12 19:10:12 config.py:542] This model supports multiple tasks: {'classify', 'score', 'generate', 'embed', 'reward'}. Defaulting to 'generate'.\n",
            "Unsloth: vLLM Bitsandbytes config using kwargs = {'load_in_8bit': False, 'load_in_4bit': True, 'bnb_4bit_compute_dtype': 'bfloat16', 'bnb_4bit_quant_storage': 'uint8', 'bnb_4bit_quant_type': 'nf4', 'bnb_4bit_use_double_quant': True, 'llm_int8_enable_fp32_cpu_offload': False, 'llm_int8_has_fp16_weight': False, 'llm_int8_skip_modules': ['lm_head', 'multi_modal_projector', 'merger', 'modality_projection'], 'llm_int8_threshold': 6.0}\n",
            "INFO 02-12 19:10:12 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.2) with config: model='unsloth/meta-llama-3.1-8b-instruct-bnb-4bit', speculative_config=None, tokenizer='unsloth/meta-llama-3.1-8b-instruct-bnb-4bit', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32000, download_dir=None, load_format=LoadFormat.BITSANDBYTES, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=bitsandbytes, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=unsloth/meta-llama-3.1-8b-instruct-bnb-4bit, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"level\":0,\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n",
            "INFO 02-12 19:10:13 cuda.py:230] Using Flash Attention backend.\n",
            "INFO 02-12 19:10:13 model_runner.py:1110] Starting to load model unsloth/meta-llama-3.1-8b-instruct-bnb-4bit...\n",
            "INFO 02-12 19:10:13 loader.py:1102] Loading weights with BitsAndBytes quantization.  May take a while ...\n",
            "INFO 02-12 19:10:14 weight_utils.py:252] Using model weights format ['*.safetensors']\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "502a4d1f11144c06b514627a9837c098"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bc7abffebcde4fb3ba06ae2310bf2896"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO 02-12 19:10:18 model_runner.py:1115] Loading model weights took 5.3541 GB\n",
            "INFO 02-12 19:10:18 punica_selector.py:18] Using PunicaWrapperGPU.\n",
            "INFO 02-12 19:10:24 worker.py:267] Memory profiling takes 4.93 seconds\n",
            "INFO 02-12 19:10:24 worker.py:267] the current vLLM instance can use total_gpu_memory (39.56GiB) x gpu_memory_utilization (0.49) = 19.57GiB\n",
            "INFO 02-12 19:10:24 worker.py:267] model weights take 5.35GiB; non_torch_memory takes 0.09GiB; PyTorch activation peak memory takes 3.43GiB; the rest of the memory reserved for KV Cache is 10.70GiB.\n",
            "INFO 02-12 19:10:24 executor_base.py:110] # CUDA blocks: 5476, # CPU blocks: 3072\n",
            "INFO 02-12 19:10:24 executor_base.py:115] Maximum concurrency for 32000 tokens per request: 2.74x\n",
            "INFO 02-12 19:10:28 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Capturing CUDA graph shapes: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 35/35 [00:45<00:00,  1.30s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO 02-12 19:11:14 model_runner.py:1562] Graph capturing finished in 45 secs, took 0.82 GiB\n",
            "INFO 02-12 19:11:14 llm_engine.py:431] init engine (profile, create kv cache, warmup model) took 55.62 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Unsloth 2025.2.5 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
          ]
        }
      ],
      "source": [
        "from unsloth import is_bfloat16_supported\n",
        "import torch\n",
        "max_seq_length = 32000 # Can increase for longer reasoning traces\n",
        "lora_rank = 64 # Larger rank = smarter, but slower\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"meta-llama/meta-Llama-3.1-8B-Instruct\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    load_in_4bit = True, # False for LoRA 16bit\n",
        "    fast_inference = True, # Enable vLLM fast inference\n",
        "    max_lora_rank = lora_rank,\n",
        "    gpu_memory_utilization = 0.5, # Reduce if out of memory\n",
        ")\n",
        "\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = lora_rank, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
        "    target_modules = [\n",
        "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
        "    ], # Remove QKVO if out of memory\n",
        "    lora_alpha = lora_rank,\n",
        "    use_gradient_checkpointing = \"unsloth\", # Enable long context finetuning\n",
        "    random_state = 3407,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Y56ln_izS9E"
      },
      "source": [
        "### Data Prep\n",
        "<a name=\"Data\"></a>\n",
        "\n",
        "We directly leverage [@willccbb](https://gist.github.com/willccbb/4676755236bb08cab5f4e54a0475d6fb) for data prep and all reward functions. You are free to create your own!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "cXk993X6C2ZZ"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import difflib\n",
        "from datasets import load_dataset, Dataset\n",
        "\n",
        "# System prompt instructing the model to output only the <reasoning> block,\n",
        "# followed immediately by the final answer in the form of a git commit message.\n",
        "SYSTEM_PROMPT = \"\"\"\n",
        "Please respond using the following format:\n",
        "<reasoning>\n",
        "Your chain-of-thought here.\n",
        "</reasoning>\n",
        "Your final answer should appear immediately after the </reasoning> tag and must be a git commit message that adheres to the following guidelines:\n",
        "\n",
        "- **Title (Subject Line):**\n",
        "  - Use the imperative mood (e.g., \"Fix bug\" not \"Fixed bug\" or \"Fixes bug\").\n",
        "  - Capitalize the first letter.\n",
        "  - Do not end with a period.\n",
        "  - Keep to a maximum of 50 characters.\n",
        "\n",
        "- **Body:**\n",
        "  - Separate from the title with a blank line.\n",
        "  - Explain the *what* and *why* of the change, not the *how*.\n",
        "  - Wrap lines at 72 characters.\n",
        "\n",
        "- **Additional Recommendations:**\n",
        "  - Use bullet points for multiple items, if necessary.\n",
        "\n",
        "Example:\n",
        "\n",
        "<reasoning>\n",
        "Analyzed the current implementation and identified an off-by-one error in the loop causing index out-of-range exceptions. Adjusted the loop condition to prevent this error.\n",
        "</reasoning>\n",
        "Fix off-by-one error in loop\n",
        "\n",
        "The loop was iterating one time too many, leading to index out-of-range exceptions. Adjusting the termination condition ensures it stays within valid bounds.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# Template for generation\n",
        "XML_COT_FORMAT = \"\"\"\\\n",
        "<reasoning>\n",
        "{reasoning}\n",
        "</reasoning>\n",
        "{answer}\n",
        "\"\"\"\n",
        "\n",
        "def extract_xml_answer(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Safely extracts the final commit message which comes after the </reasoning> tag.\n",
        "    Returns an empty string if the tag is not found.\n",
        "    \"\"\"\n",
        "    if \"</reasoning>\" not in text:\n",
        "        return \"\"\n",
        "    parts = text.split(\"</reasoning>\", 1)\n",
        "    return parts[1].strip()\n",
        "\n",
        "def extract_hash_answer(text: str) -> str | None:\n",
        "    \"\"\"\n",
        "    Extracts the expected commit message from within a git commit message code block.\n",
        "    The expected answer is assumed to be enclosed in a code block marked by ```git-commit-message and a closing ```.\n",
        "\n",
        "    Example:\n",
        "    ```git-commit-message\n",
        "    Commit message here.\n",
        "    ```\n",
        "    \"\"\"\n",
        "    marker = \"```git-commit-message\"\n",
        "    if marker not in text:\n",
        "        return None\n",
        "    try:\n",
        "        after_marker = text.split(marker, 1)[1]\n",
        "        commit_message = after_marker.split(\"```\", 1)[0].strip()\n",
        "        return commit_message\n",
        "    except IndexError:\n",
        "        return None\n",
        "\n",
        "def get_gsm8k_questions(split=\"train\") -> Dataset:\n",
        "    \"\"\"\n",
        "    Prepares the GSM8K dataset by inserting the system prompt and the user question.\n",
        "    The expected answer is extracted from a git commit message code block in the dataset entry.\n",
        "    \"\"\"\n",
        "    data = load_dataset('Tavernari/git-commit-message-dt', 'default')[split]  # type: ignore\n",
        "    data = data.map(lambda x: {  # type: ignore\n",
        "        'prompt': [\n",
        "            {'role': 'system', 'content': SYSTEM_PROMPT},\n",
        "            {'role': 'user', 'content': x['input']}\n",
        "        ],\n",
        "        'answer': extract_hash_answer(x['output'])\n",
        "    })  # type: ignore\n",
        "    return data  # type: ignore\n",
        "\n",
        "dataset = get_gsm8k_questions()\n",
        "\n",
        "# ------------------------\n",
        "# Reward Functions\n",
        "# ------------------------\n",
        "\n",
        "def extract_xml_answer(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Extracts the commit message after the </reasoning> tag.\n",
        "    \"\"\"\n",
        "    if \"</reasoning>\" not in text:\n",
        "        return \"\"\n",
        "    parts = text.split(\"</reasoning>\", 1)\n",
        "    return parts[1].strip()\n",
        "\n",
        "def contains_code_snippet(text: str) -> bool:\n",
        "    \"\"\"\n",
        "    Checks if the text contains any code snippet marked by triple backticks (```).\n",
        "    \"\"\"\n",
        "    return \"```\" in text\n",
        "\n",
        "def contains_unreadable_text(text: str) -> bool:\n",
        "    \"\"\"\n",
        "    Identifies unreadable or gibberish content.\n",
        "    - If text contains too many special characters.\n",
        "    - If text looks like base64 encoding or random strings.\n",
        "    \"\"\"\n",
        "    return bool(re.search(r'[^a-zA-Z0-9 .,\\n\\-?!:;()]+', text))\n",
        "\n",
        "def correctness_reward_func(prompts, completions, answer, **kwargs) -> list[float]:\n",
        "    \"\"\"\n",
        "    Computes correctness reward based on similarity to expected answer.\n",
        "    Penalizes responses containing snippets or unreadable content.\n",
        "    \"\"\"\n",
        "    responses = [completion[0]['content'] for completion in completions]\n",
        "    extracted_responses = [extract_xml_answer(r) for r in responses]\n",
        "    scores = []\n",
        "\n",
        "    for resp, expected in zip(extracted_responses, answer):\n",
        "        ratio = difflib.SequenceMatcher(None, resp, expected).ratio()\n",
        "        score = 2.0 * ratio\n",
        "\n",
        "        # Apply penalties\n",
        "        if contains_code_snippet(resp):\n",
        "            score -= 1.0  # Significant penalty for including code snippets\n",
        "        if contains_unreadable_text(resp):\n",
        "            score -= 0.5  # Moderate penalty for unreadable content\n",
        "\n",
        "        scores.append(max(score, 0.0))  # Ensure score is not negative\n",
        "\n",
        "    return scores\n",
        "\n",
        "def format_reward_func(completions, **kwargs) -> list[float]:\n",
        "    \"\"\"\n",
        "    Ensures commit message follows proper formatting.\n",
        "    Penalizes code snippets or unreadable text.\n",
        "    \"\"\"\n",
        "    responses = [completion[0]['content'] for completion in completions]\n",
        "    extracted_responses = [extract_xml_answer(r) for r in responses]\n",
        "    scores = []\n",
        "\n",
        "    for msg in extracted_responses:\n",
        "        lines = msg.splitlines()\n",
        "\n",
        "        if not lines:\n",
        "            scores.append(0.0)\n",
        "            continue\n",
        "\n",
        "        title = lines[0].strip()\n",
        "        if not title or len(title) > 70:\n",
        "            scores.append(0.0)\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            blank_index = lines.index('')\n",
        "        except ValueError:\n",
        "            scores.append(0.0)\n",
        "            continue\n",
        "\n",
        "        body = \"\\n\".join(lines[blank_index+1:]).strip()\n",
        "        score = 0.5 if body else 0.0\n",
        "\n",
        "        # Apply penalties\n",
        "        if contains_code_snippet(msg):\n",
        "            score -= 0.3\n",
        "        if contains_unreadable_text(msg):\n",
        "            score -= 0.2\n",
        "\n",
        "        scores.append(max(score, 0.0))\n",
        "\n",
        "    return scores\n",
        "\n",
        "def strict_format_reward_func(completions, **kwargs) -> list[float]:\n",
        "    \"\"\"\n",
        "    Checks if output strictly follows <reasoning> block and commit format.\n",
        "    \"\"\"\n",
        "    pattern = r\"^<reasoning>\\n.*?\\n</reasoning>\\n(.+)\\n\\n(.+)$\"\n",
        "    responses = [completion[0][\"content\"] for completion in completions]\n",
        "    matches = [re.match(pattern, r, re.DOTALL) for r in responses]\n",
        "    scores = [0.5 if match else 0.0 for match in matches]\n",
        "\n",
        "    for i, r in enumerate(responses):\n",
        "        if contains_code_snippet(r) or contains_unreadable_text(r):\n",
        "            scores[i] = 0.0\n",
        "\n",
        "    return scores\n",
        "\n",
        "def xmlcount_reward_func(completions, **kwargs) -> list[float]:\n",
        "    \"\"\"\n",
        "    Applies a structured scoring system for XML and commit formatting.\n",
        "    \"\"\"\n",
        "    responses = [completion[0][\"content\"] for completion in completions]\n",
        "    scores = []\n",
        "\n",
        "    for text in responses:\n",
        "        score = 0.0\n",
        "        if re.search(r\"^<reasoning>\\s*\", text):\n",
        "            score += 0.125\n",
        "        if re.search(r\"\\s*</reasoning>\\s*\", text):\n",
        "            score += 0.125\n",
        "\n",
        "        after_reasoning = \"\"\n",
        "        if \"</reasoning>\" in text:\n",
        "            after_reasoning = text.split(\"</reasoning>\", 1)[-1].strip()\n",
        "\n",
        "        if after_reasoning:\n",
        "            lines = after_reasoning.splitlines()\n",
        "            if lines and 0 < len(lines[0].strip()) <= 70:\n",
        "                score += 0.125\n",
        "            if len(lines) >= 3 and lines[1].strip() == \"\":\n",
        "                body = \"\\n\".join(lines[2:]).strip()\n",
        "                if body:\n",
        "                    score += 0.125\n",
        "\n",
        "        # Apply penalties\n",
        "        if contains_code_snippet(text):\n",
        "            score -= 0.25\n",
        "        if contains_unreadable_text(text):\n",
        "            score -= 0.25\n",
        "\n",
        "        scores.append(max(score, 0.0))\n",
        "\n",
        "    return scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bTnL_tJnzh2L"
      },
      "source": [
        "<a name=\"Train\"></a>\n",
        "### Train the model\n",
        "\n",
        "Now set up GRPO Trainer and all configurations!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ptqkXK2D4d6p",
        "outputId": "f67137c1-9325-476a-91d3-c5503a3476ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "torch.distributed process group is initialized, but parallel_mode != ParallelMode.DISTRIBUTED. In order to use Torch DDP, launch your script with `python -m torch.distributed.launch\n"
          ]
        }
      ],
      "source": [
        "from trl import GRPOConfig, GRPOTrainer\n",
        "training_args = GRPOConfig(\n",
        "    use_vllm = True, # use vLLM for fast inference!\n",
        "    learning_rate = 5e-6,\n",
        "    adam_beta1 = 0.9,\n",
        "    adam_beta2 = 0.99,\n",
        "    weight_decay = 0.1,\n",
        "    warmup_ratio = 0.1,\n",
        "    lr_scheduler_type = \"cosine\",\n",
        "    optim = \"adamw_8bit\",\n",
        "    logging_steps = 1,\n",
        "    bf16 = is_bfloat16_supported(),\n",
        "    fp16 = not is_bfloat16_supported(),\n",
        "    per_device_train_batch_size = 1,\n",
        "    gradient_accumulation_steps = 1, # Increase to 4 for smoother training\n",
        "    num_generations = 8, # Decrease if out of memory\n",
        "    max_prompt_length = 1024,\n",
        "    max_completion_length = 1024,\n",
        "    # num_train_epochs = 1, # Set to 1 for a full training run\n",
        "    max_steps = 250,\n",
        "    save_steps = 250,\n",
        "    max_grad_norm = 0.1,\n",
        "    report_to = \"none\", # Can use Weights & Biases\n",
        "    output_dir = \"outputs\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X_71Y0eKz5yE"
      },
      "source": [
        "And let's run the trainer! If you scroll up, you'll see a table of rewards. The goal is to see the `reward` column increase!\n",
        "\n",
        "You might have to wait 150 to 200 steps for any action. You'll probably get 0 reward for the first 100 steps. Please be patient!\n",
        "\n",
        "| Step | Training Loss | reward    | reward_std | completion_length | kl       |\n",
        "|------|---------------|-----------|------------|-------------------|----------|\n",
        "| 1    | 0.000000      | 0.125000  | 0.000000   | 200.000000        | 0.000000 |\n",
        "| 2    | 0.000000      | 0.072375  | 0.248112   | 200.000000        | 0.000000 |\n",
        "| 3    | 0.000000      | -0.079000 | 0.163776   | 182.500000        | 0.000005 |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vzOuSVCL_GA9",
        "outputId": "75ce74ad-382c-4a22-950c-8067c71987ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
            "   \\\\   /|    Num examples = 2,535 | Num Epochs = 1\n",
            "O^O/ \\_/ \\    Batch size per device = 1 | Gradient Accumulation steps = 1\n",
            "\\        /    Total batch size = 1 | Total steps = 250\n",
            " \"-____-\"     Number of trainable parameters = 167,772,160\n"
          ]
        }
      ],
      "source": [
        "trainer = GRPOTrainer(\n",
        "    model = model,\n",
        "    processing_class = tokenizer,\n",
        "    reward_funcs = [\n",
        "        xmlcount_reward_func,\n",
        "        soft_format_reward_func,\n",
        "        strict_format_reward_func,\n",
        "        int_reward_func,\n",
        "        correctness_reward_func,\n",
        "    ],\n",
        "    args = training_args,\n",
        "    train_dataset = dataset,\n",
        ")\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yUbluAAhD0Lg"
      },
      "source": [
        "<a name=\"Inference\"></a>\n",
        "### Inference\n",
        "Now let's try the model we just trained! First, let's first try the model without any GRPO trained:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IqzsdZzeDM_m"
      },
      "outputs": [],
      "source": [
        "text = tokenizer.apply_chat_template([\n",
        "    {\"role\" : \"user\", \"content\" : \"diff --git a/Project/Scenes/Browse/InteractiveSchedule/Vertical Collection/Browse.InteractiveSchedule.VerticalCollection+ViewController.swift b/Project/Scenes/Browse/InteractiveSchedule/Vertical Collection/Browse.InteractiveSchedule.VerticalCollection+ViewController.swift --- a/Project/Scenes/Browse/InteractiveSchedule/Vertical Collection/Browse.InteractiveSchedule.VerticalCollection+ViewController.swift +++ b/Project/Scenes/Browse/InteractiveSchedule/Vertical Collection/Browse.InteractiveSchedule.VerticalCollection+ViewController.swift @@ -18,10 +18,9 @@ private enum Constants { - static let invalidateCellDelay: TimeInterval = 0.1 + static let invalidateCellDebounceTime: TimeInterval = 0.05 static let topCellOffset: Double = 60 } private var focusState: FocusState = .into - private var scrollingDirection: ScrollingDirection? = .none private var animateAfterPosY: Double? @@ -166,17 +165,8 @@ func scrollViewDidScroll(_ scrollView: UIScrollView) { guard let currentFocusedIndexPath, - let scrollingDirection, shouldInvalidate(yPos: scrollView.contentOffset.y) else { return } - let delay: Double = switch scrollingDirection { - case .down: - 0 - case .up: - Constants.invalidateCellDelay - } - - invalidateCellForFocusedIndexPath(currentFocusedIndexPath, after: delay) - self.scrollingDirection = .none + invalidateCellForFocusedIndexPath(currentFocusedIndexPath) self.animateAfterPosY = nil } @@ -193,15 +183,10 @@ let scrollDirectionDifference = targetOffset - scrollView.contentOffset.y - scrollingDirection = scrollDirectionDifference > 0 ? .down : .up animateAfterPosY = scrollView.contentOffset.y + scrollDirectionDifference / 2 } - func invalidateCellForFocusedIndexPath(_ indexPath: IndexPath, after duration: TimeInterval) { + func invalidateCellForFocusedIndexPath(_ indexPath: IndexPath) { debounceInvalidateCellWorkItem?.cancel() - guard duration > .zero else { - collectionView.cellForItem(at: indexPath)?.invalidateIntrinsicContentSize() - return - } - + let workItem = DispatchWorkItem { [weak self] in self?.collectionView.cellForItem(at: indexPath)?.invalidateIntrinsicContentSize() @@ -210,5 +195,5 @@ debounceInvalidateCellWorkItem = workItem DispatchQueue.main.asyncAfter( - deadline: .now() + duration, + deadline: .now() + Constants.invalidateCellDebounceTime, execute: workItem )\"},\n",
        "], tokenize = False, add_generation_prompt = True)\n",
        "\n",
        "from vllm import SamplingParams\n",
        "sampling_params = SamplingParams(\n",
        "    temperature = 0.8,\n",
        "    top_p = 0.95,\n",
        "    max_tokens = 1024,\n",
        ")\n",
        "output = model.fast_generate(\n",
        "    [text],\n",
        "    sampling_params = sampling_params,\n",
        "    lora_request = None,\n",
        ")[0].outputs[0].text\n",
        "\n",
        "output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G4lzJD7REFjs"
      },
      "source": [
        "And now with the LoRA we just trained with GRPO - we first save the LoRA first!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YC9BBT0RESln"
      },
      "outputs": [],
      "source": [
        "model.save_lora(\"grpo_saved_lora\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LherO2vzEbMt"
      },
      "source": [
        "Now we load the LoRA and test:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SDKIhhvN6lAF"
      },
      "outputs": [],
      "source": [
        "text = tokenizer.apply_chat_template([\n",
        "    {\"role\" : \"system\", \"content\" : SYSTEM_PROMPT},\n",
        "    {\"role\" : \"user\", \"content\" : \"diff --git a/Project/Scenes/Browse/InteractiveSchedule/Vertical Collection/Browse.InteractiveSchedule.VerticalCollection+ViewController.swift b/Project/Scenes/Browse/InteractiveSchedule/Vertical Collection/Browse.InteractiveSchedule.VerticalCollection+ViewController.swift --- a/Project/Scenes/Browse/InteractiveSchedule/Vertical Collection/Browse.InteractiveSchedule.VerticalCollection+ViewController.swift +++ b/Project/Scenes/Browse/InteractiveSchedule/Vertical Collection/Browse.InteractiveSchedule.VerticalCollection+ViewController.swift @@ -18,10 +18,9 @@ private enum Constants { - static let invalidateCellDelay: TimeInterval = 0.1 + static let invalidateCellDebounceTime: TimeInterval = 0.05 static let topCellOffset: Double = 60 } private var focusState: FocusState = .into - private var scrollingDirection: ScrollingDirection? = .none private var animateAfterPosY: Double? @@ -166,17 +165,8 @@ func scrollViewDidScroll(_ scrollView: UIScrollView) { guard let currentFocusedIndexPath, - let scrollingDirection, shouldInvalidate(yPos: scrollView.contentOffset.y) else { return } - let delay: Double = switch scrollingDirection { - case .down: - 0 - case .up: - Constants.invalidateCellDelay - } - - invalidateCellForFocusedIndexPath(currentFocusedIndexPath, after: delay) - self.scrollingDirection = .none + invalidateCellForFocusedIndexPath(currentFocusedIndexPath) self.animateAfterPosY = nil } @@ -193,15 +183,10 @@ let scrollDirectionDifference = targetOffset - scrollView.contentOffset.y - scrollingDirection = scrollDirectionDifference > 0 ? .down : .up animateAfterPosY = scrollView.contentOffset.y + scrollDirectionDifference / 2 } - func invalidateCellForFocusedIndexPath(_ indexPath: IndexPath, after duration: TimeInterval) { + func invalidateCellForFocusedIndexPath(_ indexPath: IndexPath) { debounceInvalidateCellWorkItem?.cancel() - guard duration > .zero else { - collectionView.cellForItem(at: indexPath)?.invalidateIntrinsicContentSize() - return - } - + let workItem = DispatchWorkItem { [weak self] in self?.collectionView.cellForItem(at: indexPath)?.invalidateIntrinsicContentSize() @@ -210,5 +195,5 @@ debounceInvalidateCellWorkItem = workItem DispatchQueue.main.asyncAfter( - deadline: .now() + duration, + deadline: .now() + Constants.invalidateCellDebounceTime, execute: workItem )\"},\n",
        "], tokenize = False, add_generation_prompt = True)\n",
        "\n",
        "from vllm import SamplingParams\n",
        "sampling_params = SamplingParams(\n",
        "    temperature = 0.8,\n",
        "    top_p = 0.95,\n",
        "    max_tokens = 1024,\n",
        ")\n",
        "output = model.fast_generate(\n",
        "    text,\n",
        "    sampling_params = sampling_params,\n",
        "    lora_request = model.load_lora(\"grpo_saved_lora\"),\n",
        ")[0].outputs[0].text\n",
        "\n",
        "output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ZBnvg2f9Nlg"
      },
      "source": [
        "Our reasoning model is much better - it's not always correct, since we only trained it for an hour or so - it'll be better if we extend the sequence length and train for longer!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RphEZRSfFhru"
      },
      "source": [
        "<a name=\"Save\"></a>\n",
        "### Saving to float16 for VLLM\n",
        "\n",
        "We also support saving to `float16` directly. Select `merged_16bit` for float16 or `merged_4bit` for int4. We also allow `lora` adapters as a fallback. Use `push_to_hub_merged` to upload to your Hugging Face account! You can go to https://huggingface.co/settings/tokens for your personal tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GwNY9_PrFiXZ"
      },
      "outputs": [],
      "source": [
        "# Merge to 16bit\n",
        "if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_16bit\",)\n",
        "if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"merged_16bit\", token = \"\")\n",
        "\n",
        "# Merge to 4bit\n",
        "if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_4bit\",)\n",
        "if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"merged_4bit\", token = \"\")\n",
        "\n",
        "# Just LoRA adapters\n",
        "if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"lora\",)\n",
        "if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"lora\", token = \"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BDUGPiL3Fkkq"
      },
      "source": [
        "### GGUF / llama.cpp Conversion\n",
        "To save to `GGUF` / `llama.cpp`, we support it natively now! We clone `llama.cpp` and we default save it to `q8_0`. We allow all methods like `q4_k_m`. Use `save_pretrained_gguf` for local saving and `push_to_hub_gguf` for uploading to HF.\n",
        "\n",
        "Some supported quant methods (full list on our [Wiki page](https://github.com/unslothai/unsloth/wiki#gguf-quantization-options)):\n",
        "* `q8_0` - Fast conversion. High resource use, but generally acceptable.\n",
        "* `q4_k_m` - Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q4_K.\n",
        "* `q5_k_m` - Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q5_K.\n",
        "\n",
        "[**NEW**] To finetune and auto export to Ollama, try our [Ollama notebook](https://colab.research.google.com/drive/1WZDi7APtQ9VsvOrQSSC5DDtxq159j8iZ?usp=sharing)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AGo4dbWvFk4M"
      },
      "outputs": [],
      "source": [
        "# Save to 8bit Q8_0\n",
        "if False: model.save_pretrained_gguf(\"model\", tokenizer,)\n",
        "# Remember to go to https://huggingface.co/settings/tokens for a token!\n",
        "# And change hf to your username!\n",
        "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, token = \"\")\n",
        "\n",
        "# Save to 16bit GGUF\n",
        "if False: model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"f16\")\n",
        "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"f16\", token = \"\")\n",
        "\n",
        "# Save to q4_k_m GGUF\n",
        "if False: model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"q4_k_m\")\n",
        "if True: model.push_to_hub_gguf(\"Tavernari/git-commit-message\", tokenizer, quantization_method = \"q4_k_m\", token = \"\")\n",
        "\n",
        "# Save to multiple GGUF options - much faster if you want multiple!\n",
        "if False:\n",
        "    model.push_to_hub_gguf(\n",
        "        \"Tavernari/git-commit-message\", # Change hf to your username!\n",
        "        tokenizer,\n",
        "        quantization_method = [\"q4_k_m\", \"q8_0\", \"q5_k_m\",],\n",
        "        token = \"\",\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZW99WXVP2oQA"
      },
      "source": [
        "Now, use the `model-unsloth.gguf` file or `model-unsloth-Q4_K_M.gguf` file in llama.cpp or a UI based system like Jan or Open WebUI. You can install Jan [here](https://github.com/janhq/jan) and Open WebUI [here](https://github.com/open-webui/open-webui)\n",
        "\n",
        "And we're done! If you have any questions on Unsloth, we have a [Discord](https://discord.gg/unsloth) channel! If you find any bugs or want to keep updated with the latest LLM stuff, or need help, join projects etc, feel free to join our Discord!\n",
        "\n",
        "Some other links:\n",
        "1. Llama 3.2 Conversational notebook. [Free Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_(1B_and_3B)-Conversational.ipynb)\n",
        "2. Saving finetunes to Ollama. [Free notebook](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3_(8B)-Ollama.ipynb)\n",
        "3. Llama 3.2 Vision finetuning - Radiography use case. [Free Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_(11B)-Vision.ipynb)\n",
        "6. See notebooks for DPO, ORPO, Continued pretraining, conversational finetuning and more on our [documentation](https://docs.unsloth.ai/get-started/unsloth-notebooks)!\n",
        "\n",
        "<div class=\"align-center\">\n",
        "  <a href=\"https://unsloth.ai\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n",
        "  <a href=\"https://discord.gg/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord.png\" width=\"145\"></a>\n",
        "  <a href=\"https://docs.unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/blob/main/images/documentation%20green%20button.png?raw=true\" width=\"125\"></a>\n",
        "\n",
        "  Join Discord if you need help + ‚≠êÔ∏è <i>Star us on <a href=\"https://github.com/unslothai/unsloth\">Github</a> </i> ‚≠êÔ∏è\n",
        "</div>\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "502a4d1f11144c06b514627a9837c098": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4d6b2701a4904a3caefb5a61de8604fe",
              "IPY_MODEL_25b02faaa37543fab9cd5cf4d9ded89c",
              "IPY_MODEL_9de1ced1719d4e93bd970f4734b5b4ef"
            ],
            "layout": "IPY_MODEL_b5a517d5225a4d5a968b7dac1f9d1c53"
          }
        },
        "4d6b2701a4904a3caefb5a61de8604fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_739d502c79404f438b7e5cf0d69ed710",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_7e85a2be6ec84cfd990bef6f2c7fc0eb",
            "value": ""
          }
        },
        "25b02faaa37543fab9cd5cf4d9ded89c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5d0e808b54324eedb7c55f32b6551647",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_30c15883925d4a8dbd504abf6b7dff0f",
            "value": 1
          }
        },
        "9de1ced1719d4e93bd970f4734b5b4ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7984a46caf4d4fd7880132d386ffdfee",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_ccfd6b0f8d034af9b4b82dd35db23aa6",
            "value": "Loading‚Äásafetensors‚Äácheckpoint‚Äáshards:‚Äá100%‚ÄáCompleted‚Äá|‚Äá1/1‚Äá[00:01&lt;00:00,‚Äá‚Äá1.85s/it]\n"
          }
        },
        "b5a517d5225a4d5a968b7dac1f9d1c53": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "739d502c79404f438b7e5cf0d69ed710": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7e85a2be6ec84cfd990bef6f2c7fc0eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5d0e808b54324eedb7c55f32b6551647": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "30c15883925d4a8dbd504abf6b7dff0f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7984a46caf4d4fd7880132d386ffdfee": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ccfd6b0f8d034af9b4b82dd35db23aa6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bc7abffebcde4fb3ba06ae2310bf2896": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_32992b80b6074eb880f08eddbc2cbebc",
              "IPY_MODEL_cf3ca4b73f444f5183030d2696fee992",
              "IPY_MODEL_bca0ce2597954f5aa1520d03371fc52b"
            ],
            "layout": "IPY_MODEL_a6f550d797b8403880b725d1f95464f5"
          }
        },
        "32992b80b6074eb880f08eddbc2cbebc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3787c8be64414d08b1b3e595440b9ee2",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_8f31da0f871c48c785224f74162d1f59",
            "value": ""
          }
        },
        "cf3ca4b73f444f5183030d2696fee992": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cb58b76dc83a42658a8bbc4dd6ed5bb2",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3663dc1e8e454d2ba17fc90e0d809d67",
            "value": 1
          }
        },
        "bca0ce2597954f5aa1520d03371fc52b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_56d47b464e27400a9ae384255a96ed76",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_359e37cc82344a10824d1a7c24d819e9",
            "value": "Loading‚Äásafetensors‚Äácheckpoint‚Äáshards:‚Äá100%‚ÄáCompleted‚Äá|‚Äá1/1‚Äá[00:01&lt;00:00,‚Äá‚Äá1.91s/it]\n"
          }
        },
        "a6f550d797b8403880b725d1f95464f5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3787c8be64414d08b1b3e595440b9ee2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8f31da0f871c48c785224f74162d1f59": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cb58b76dc83a42658a8bbc4dd6ed5bb2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3663dc1e8e454d2ba17fc90e0d809d67": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "56d47b464e27400a9ae384255a96ed76": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "359e37cc82344a10824d1a7c24d819e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}