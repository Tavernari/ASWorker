{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zLirmIpom8Xm"
      },
      "source": [
        "To run this, press \"*Runtime*\" and press \"*Run all*\" on a **free** Tesla T4 Google Colab instance!\n",
        "<div class=\"align-center\">\n",
        "<a href=\"https://unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n",
        "<a href=\"https://discord.gg/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord button.png\" width=\"145\"></a>\n",
        "<a href=\"https://docs.unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/blob/main/images/documentation%20green%20button.png?raw=true\" width=\"125\"></a></a> Join Discord if you need help + ⭐ <i>Star us on <a href=\"https://github.com/unslothai/unsloth\">Github</a> </i> ⭐\n",
        "</div>\n",
        "\n",
        "To install Unsloth on your own computer, follow the installation instructions on our Github page [here](https://docs.unsloth.ai/get-started/installing-+-updating).\n",
        "\n",
        "You will learn how to do [data prep](#Data), how to [train](#Train), how to [run the model](#Inference), & [how to save it](#Save)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W9o0CW9Zm8Xo"
      },
      "source": [
        "### News"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_zTqh6J5m8Xo"
      },
      "source": [
        "**Read our [blog post](https://unsloth.ai/blog/r1-reasoning) for guidance on how to train reasoning models.**\n",
        "\n",
        "Visit our docs for all our [model uploads](https://docs.unsloth.ai/get-started/all-our-models) and [notebooks](https://docs.unsloth.ai/get-started/unsloth-notebooks).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-MNGcsQsm8Xp"
      },
      "source": [
        "### Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mTsmVZVhm8Xp"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# Skip restarting message in Colab\n",
        "import sys; modules = list(sys.modules.keys())\n",
        "for x in modules: sys.modules.pop(x) if \"PIL\" in x or \"google\" in x else None\n",
        "\n",
        "!pip install unsloth vllm\n",
        "!pip install --upgrade pillow\n",
        "# If you are running this notebook on local, you need to install `diffusers` too\n",
        "# !pip install diffusers\n",
        "# Temporarily install a specific TRL nightly version\n",
        "!pip install git+https://github.com/huggingface/trl.git@e95f9fb74a3c3647b86f251b7e230ec51c64b72b"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "venx8rH3m8Xp"
      },
      "source": [
        "### Unsloth"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P1zyu9Ug2XEt"
      },
      "source": [
        "Use `PatchFastRL` before all functions to patch GRPO and other RL algorithms!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "59DIs5BMcvjN"
      },
      "outputs": [],
      "source": [
        "from unsloth import FastLanguageModel, PatchFastRL\n",
        "PatchFastRL(\"GRPO\", FastLanguageModel)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R8-SLRUB2gwM"
      },
      "source": [
        "Load up `Llama 3.1 8B Instruct`, and set parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DkIvEkIIkEyB"
      },
      "outputs": [],
      "source": [
        "from unsloth import is_bfloat16_supported\n",
        "import torch\n",
        "max_seq_length = 1024 # Can increase for longer reasoning traces\n",
        "lora_rank = 64 # Larger rank = smarter, but slower\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"meta-llama/meta-Llama-3.1-8B-Instruct\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    load_in_4bit = True, # False for LoRA 16bit\n",
        "    fast_inference = True, # Enable vLLM fast inference\n",
        "    max_lora_rank = lora_rank,\n",
        "    gpu_memory_utilization = 0.6, # Reduce if out of memory\n",
        ")\n",
        "\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = lora_rank, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
        "    target_modules = [\n",
        "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
        "    ], # Remove QKVO if out of memory\n",
        "    lora_alpha = lora_rank,\n",
        "    use_gradient_checkpointing = \"unsloth\", # Enable long context finetuning\n",
        "    random_state = 3407,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7KGgPgk_5S8r"
      },
      "source": [
        "### Data Prep\n",
        "<a name=\"Data\"></a>\n",
        "\n",
        "We directly leverage [@willccbb](https://gist.github.com/willccbb/4676755236bb08cab5f4e54a0475d6fb) for data prep and all reward functions. You are free to create your own!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cXk993X6C2ZZ"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from datasets import load_dataset, Dataset\n",
        "\n",
        "# Load and prep dataset\n",
        "SYSTEM_PROMPT = \"\"\"\n",
        "Given a git diff or patch you must analize and write a commit message.\n",
        "The git commit message title should have max 60-character.\n",
        "Let an empty line between title and description.\n",
        "The git commit description should only have readable content, not code.\n",
        "\n",
        "Respond in the following format:\n",
        "<reasoning>\n",
        "...\n",
        "</reasoning>\n",
        "<answer>\n",
        "...\n",
        "</answer>\n",
        "\"\"\"\n",
        "\n",
        "XML_COT_FORMAT = \"\"\"\\\n",
        "<reasoning>\n",
        "{reasoning}\n",
        "</reasoning>\n",
        "<answer>\n",
        "{answer}\n",
        "</answer>\n",
        "\"\"\"\n",
        "\n",
        "def extract_xml_answer(text: str) -> str:\n",
        "    answer = text.split(\"<answer>\")[-1]\n",
        "    answer = answer.split(\"</answer>\")[0]\n",
        "    return answer.strip()\n",
        "\n",
        "def extract_hash_answer(text: str) -> str | None:\n",
        "    return text.replace(\"```git-commit-message\", \"\").replace(\"```\", \"\").strip()\n",
        "\n",
        "# uncomment middle messages for 1-shot prompting\n",
        "def get_gsm8k_questions(split = \"train\") -> Dataset:\n",
        "    data = load_dataset('Tavernari/git-commit-message', 'default')[split] # type: ignore\n",
        "    data = data.map(lambda x: { # type: ignore\n",
        "        'prompt': [\n",
        "            {'role': 'system', 'content': SYSTEM_PROMPT},\n",
        "            {'role': 'user', 'content': x['input']}\n",
        "        ],\n",
        "        'answer': extract_hash_answer(x['output'])\n",
        "    }) # type: ignore\n",
        "    return data # type: ignore\n",
        "\n",
        "dataset = get_gsm8k_questions()\n",
        "\n",
        "# Reward functions\n",
        "def correctness_reward_func(prompts, completions, answer, **kwargs) -> list[float]:\n",
        "    responses = [completion[0]['content'] for completion in completions]\n",
        "    q = prompts[0][-1]['content']\n",
        "    extracted_responses = [extract_xml_answer(r) for r in responses]\n",
        "    print('-'*20, f\"Question:\\n{q}\", f\"\\nAnswer:\\n{answer[0]}\", f\"\\nResponse:\\n{responses[0]}\", f\"\\nExtracted:\\n{extracted_responses[0]}\")\n",
        "    return [2.0 if r == a else 0.0 for r, a in zip(extracted_responses, answer)]\n",
        "\n",
        "def int_reward_func(completions, **kwargs) -> list[float]:\n",
        "    responses = [completion[0]['content'] for completion in completions]\n",
        "    extracted_responses = [extract_xml_answer(r) for r in responses]\n",
        "    return [0.5 if r.isdigit() else 0.0 for r in extracted_responses]\n",
        "\n",
        "def strict_format_reward_func(completions, **kwargs) -> list[float]:\n",
        "    \"\"\"Reward function that checks if the completion has a specific format.\"\"\"\n",
        "    pattern = r\"^<reasoning>\\n.*?\\n</reasoning>\\n<answer>\\n.*?\\n</answer>\\n$\"\n",
        "    responses = [completion[0][\"content\"] for completion in completions]\n",
        "    matches = [re.match(pattern, r) for r in responses]\n",
        "    return [0.5 if match else 0.0 for match in matches]\n",
        "\n",
        "def soft_format_reward_func(completions, **kwargs) -> list[float]:\n",
        "    \"\"\"Reward function that checks if the completion has a specific format.\"\"\"\n",
        "    pattern = r\"<reasoning>.*?</reasoning>\\s*<answer>.*?</answer>\"\n",
        "    responses = [completion[0][\"content\"] for completion in completions]\n",
        "    matches = [re.match(pattern, r) for r in responses]\n",
        "    return [0.5 if match else 0.0 for match in matches]\n",
        "\n",
        "def count_xml(text) -> float:\n",
        "    count = 0.0\n",
        "    if text.count(\"<reasoning>\\n\") == 1:\n",
        "        count += 0.125\n",
        "    if text.count(\"\\n</reasoning>\\n\") == 1:\n",
        "        count += 0.125\n",
        "    if text.count(\"\\n<answer>\\n\") == 1:\n",
        "        count += 0.125\n",
        "        count -= len(text.split(\"\\n</answer>\\n\")[-1])*0.001\n",
        "    if text.count(\"\\n</answer>\") == 1:\n",
        "        count += 0.125\n",
        "        count -= (len(text.split(\"\\n</answer>\")[-1]) - 1)*0.001\n",
        "    return count\n",
        "\n",
        "def xmlcount_reward_func(completions, **kwargs) -> list[float]:\n",
        "    contents = [completion[0][\"content\"] for completion in completions]\n",
        "    return [count_xml(c) for c in contents]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ux6iqP7z5YOo"
      },
      "source": [
        "<a name=\"Train\"></a>\n",
        "### Train the model\n",
        "\n",
        "Now set up GRPO Trainer and all configurations!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ptqkXK2D4d6p"
      },
      "outputs": [],
      "source": [
        "from trl import GRPOConfig, GRPOTrainer\n",
        "training_args = GRPOConfig(\n",
        "    use_vllm = True, # use vLLM for fast inference!\n",
        "    learning_rate = 5e-6,\n",
        "    adam_beta1 = 0.9,\n",
        "    adam_beta2 = 0.99,\n",
        "    weight_decay = 0.1,\n",
        "    warmup_ratio = 0.1,\n",
        "    lr_scheduler_type = \"cosine\",\n",
        "    optim = \"paged_adamw_8bit\",\n",
        "    logging_steps = 1,\n",
        "    bf16 = is_bfloat16_supported(),\n",
        "    fp16 = not is_bfloat16_supported(),\n",
        "    per_device_train_batch_size = 1,\n",
        "    gradient_accumulation_steps = 1, # Increase to 4 for smoother training\n",
        "    num_generations = 6, # Decrease if out of memory\n",
        "    max_prompt_length = 4096,\n",
        "    max_completion_length = 4096,\n",
        "    num_train_epochs = 3, # Set to 1 for a full training run\n",
        "    max_steps = 240,\n",
        "    save_steps = 240,\n",
        "    max_grad_norm = 0.1,\n",
        "    report_to = \"none\", # Can use Weights & Biases\n",
        "    output_dir = \"outputs\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r9Mv8UZO5hz-"
      },
      "source": [
        "And let's run the trainer! If you scroll up, you'll see a table of rewards. The goal is to see the `reward` column increase!\n",
        "\n",
        "You might have to wait 150 to 200 steps for any action. You'll probably get 0 reward for the first 100 steps. Please be patient!\n",
        "\n",
        "| Step | Training Loss | reward    | reward_std | completion_length | kl       |\n",
        "|------|---------------|-----------|------------|-------------------|----------|\n",
        "| 1    | 0.000000      | 0.125000  | 0.000000   | 200.000000        | 0.000000 |\n",
        "| 2    | 0.000000      | 0.072375  | 0.248112   | 200.000000        | 0.000000 |\n",
        "| 3    | 0.000000      | -0.079000 | 0.163776   | 182.500000        | 0.000005 |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vzOuSVCL_GA9"
      },
      "outputs": [],
      "source": [
        "trainer = GRPOTrainer(\n",
        "    model = model,\n",
        "    processing_class = tokenizer,\n",
        "    reward_funcs = [\n",
        "        xmlcount_reward_func,\n",
        "        soft_format_reward_func,\n",
        "        strict_format_reward_func,\n",
        "        int_reward_func,\n",
        "        correctness_reward_func,\n",
        "    ],\n",
        "    args = training_args,\n",
        "    train_dataset = dataset,\n",
        ")\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tlaUdxC_VHpz"
      },
      "source": [
        "<a name=\"Inference\"></a>\n",
        "### Inference\n",
        "Now let's try the model we just trained! First, let's first try the model without any GRPO trained:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qtcz_lpbVC92"
      },
      "outputs": [],
      "source": [
        "text = tokenizer.apply_chat_template([\n",
        "    {\"role\" : \"user\", \"content\" : \"\"\"\\\n",
        "    diff --git forkSrcPrefix/ProjectTests/Tests/Configuration/ConfigurationTests.swift forkDstPrefix/ProjectTests/Tests/Configuration/ConfigurationTests.swift\n",
        "new file mode 100644\n",
        "index 0000000000000000000000000000000000000000..43875ea954cfda9cb9b28cded3c45473680c3382\n",
        "--- /dev/null\n",
        "+++ forkDstPrefix/ProjectTests/Tests/Configuration/ConfigurationTests.swift\n",
        "@@ -0,0 +1,45 @@\n",
        "+@testable import Project\n",
        "+import XCTest\n",
        "+\n",
        "+class ConfigurationTests: XCTestCase {\n",
        "+    private let decoder = JSONDecoder()\n",
        "+\n",
        "+    // MARK: - Tests\n",
        "+    func test_ConfigurationDecoding_WhenProfileCacheValuesWasProvided_UsesProvidedValues() throws {\n",
        "+        // Given\n",
        "+        let json = \"\"\"\n",
        "+        {\n",
        "+            \"version\": \"1.0.0\",\n",
        "+            \"profileWithoutPinDuration\": 3600,\n",
        "+            \"profileWithPinDuration\": 86400\n",
        "+        }\n",
        "+        \"\"\"\n",
        "+\n",
        "+        // When\n",
        "+        let configuration = try decoder.decode(Configuration.self, from: Data(json.utf8))\n",
        "+\n",
        "+        // Then\n",
        "+        XCTAssertEqual(configuration.version, \"1.0.0\")\n",
        "+        XCTAssertEqual(configuration.profileWithoutPinDuration, 3600)\n",
        "+        XCTAssertEqual(configuration.profileWithPinDuration, 86400)\n",
        "+    }\n",
        "+\n",
        "+    func test_ConfigurationDecoding_WhenProfileCacheValuesNotProvided_UsesDefaultValue() throws {\n",
        "+        // Given\n",
        "+        let json = \"\"\"\n",
        "+        {\n",
        "+            \"version\": \"1.1.0\"\n",
        "+        }\n",
        "+        \"\"\"\n",
        "+\n",
        "+        // When\n",
        "+        let configuration = try decoder.decode(Configuration.self, from: Data(json.utf8))\n",
        "+\n",
        "+        // Then\n",
        "+        XCTAssertEqual(configuration.version, \"1.1.0\")\n",
        "+        /// Default value is 30 minutes\n",
        "+        XCTAssertEqual(configuration.profileWithoutPinDuration, 1800)\n",
        "+        /// Default value is 12 hours\n",
        "+        XCTAssertEqual(configuration.profileWithPinDuration, 43200)\n",
        "+    }\n",
        "+}\n",
        "diff --git forkSrcPrefix/Project.xcodeproj/project.pbxproj forkDstPrefix/Project.xcodeproj/project.pbxproj\n",
        "index 61b65d35abf98eda4c0845782bb0f1b58d5c89e5..f28dc9c7fae76b2076d59f25043974a39b32c68d 100644\n",
        "--- forkSrcPrefix/Project.xcodeproj/project.pbxproj\n",
        "+++ forkDstPrefix/Project.xcodeproj/project.pbxproj\n",
        "@@ -2767,6 +2767,7 @@\n",
        " \t\tF3D3D8D32AB8A5F000891143 /* BrowseViewController+DisplayInvalidatedCache.swift in Sources */ = {isa = PBXBuildFile; fileRef = F3D3D8D22AB8A5F000891143 /* BrowseViewController+DisplayInvalidatedCache.swift */; };\n",
        " \t\tF3D3D8F32AB9F1BE00891143 /* InfoStorage+TokenValidityManager.swift in Sources */ = {isa = PBXBuildFile; fileRef = F3D3D8F22AB9F1BE00891143 /* InfoStorage+TokenValidityManager.swift */; };\n",
        " \t\tF3D3D8F82AB9F2C700891143 /* InfoStorageTokenExpirationManagerTests.swift in Sources */ = {isa = PBXBuildFile; fileRef = F3D3D8F72AB9F2C700891143 /* InfoStorageTokenExpirationManagerTests.swift */; };\n",
        "+\t\tF3D7C5C52D5B60BD00D8AA5D /* ConfigurationTests.swift in Sources */ = {isa = PBXBuildFile; fileRef = F3D7C5C42D5B60BD00D8AA5D /* ConfigurationTests.swift */; };\n",
        " \t\tF3DD7E322BA86CF8003B7301 /* Dependency+DispatchQueueServiceName.swift in Sources */ = {isa = PBXBuildFile; fileRef = F3DD7E312BA86CF8003B7301 /* Dependency+DispatchQueueServiceName.swift */; };\n",
        " \t\tF3DFC0192C93387900FA7E2E /* EPGAssetModel.swift in Sources */ = {isa = PBXBuildFile; fileRef = F3DFC0182C93387900FA7E2E /* EPGAssetModel.swift */; };\n",
        " \t\tF3E04CDB2C063C480054E143 /* Browse.InteractiveSchedule+ViewController+DataDisplaying.swift in Sources */ = {isa = PBXBuildFile; fileRef = F3E04CDA2C063C480054E143 /* Browse.InteractiveSchedule+ViewController+DataDisplaying.swift */; };\n",
        "@@ -6495,6 +6496,7 @@\n",
        " \t\tF3D3D8D22AB8A5F000891143 /* BrowseViewController+DisplayInvalidatedCache.swift */ = {isa = PBXFileReference; lastKnownFileType = sourcecode.swift; path = \"BrowseViewController+DisplayInvalidatedCache.swift\"; sourceTree = \"<group>\"; };\n",
        " \t\tF3D3D8F22AB9F1BE00891143 /* InfoStorage+TokenValidityManager.swift */ = {isa = PBXFileReference; lastKnownFileType = sourcecode.swift; path = \"InfoStorage+TokenValidityManager.swift\"; sourceTree = \"<group>\"; };\n",
        " \t\tF3D3D8F72AB9F2C700891143 /* InfoStorageTokenExpirationManagerTests.swift */ = {isa = PBXFileReference; lastKnownFileType = sourcecode.swift; path = InfoStorageTokenExpirationManagerTests.swift; sourceTree = \"<group>\"; };\n",
        "+\t\tF3D7C5C42D5B60BD00D8AA5D /* ConfigurationTests.swift */ = {isa = PBXFileReference; lastKnownFileType = sourcecode.swift; path = ConfigurationTests.swift; sourceTree = \"<group>\"; };\n",
        " \t\tF3DD7E312BA86CF8003B7301 /* Dependency+DispatchQueueServiceName.swift */ = {isa = PBXFileReference; lastKnownFileType = sourcecode.swift; path = \"Dependency+DispatchQueueServiceName.swift\"; sourceTree = \"<group>\"; };\n",
        " \t\tF3DFC0182C93387900FA7E2E /* EPGAssetModel.swift */ = {isa = PBXFileReference; lastKnownFileType = sourcecode.swift; path = EPGAssetModel.swift; sourceTree = \"<group>\"; };\n",
        " \t\tF3E04CDA2C063C480054E143 /* Browse.InteractiveSchedule+ViewController+DataDisplaying.swift */ = {isa = PBXFileReference; lastKnownFileType = sourcecode.swift; path = \"Browse.InteractiveSchedule+ViewController+DataDisplaying.swift\"; sourceTree = \"<group>\"; };\n",
        "@@ -15192,6 +15194,7 @@\n",
        " \t\tC412C10929F162D5003C2D36 /* Configuration */ = {\n",
        " \t\t\tisa = PBXGroup;\n",
        " \t\t\tchildren = (\n",
        "+\t\t\t\tF3D7C5C42D5B60BD00D8AA5D /* ConfigurationTests.swift */,\n",
        " \t\t\t\tC412C10A29F162DC003C2D36 /* ConfigurationVideoQualitySettingsTests.swift */,\n",
        " \t\t\t);\n",
        " \t\t\tpath = Configuration;\n",
        "@@ -22326,6 +22329,7 @@\n",
        " \t\t\t\tA5E557262637264C00ED2159 /* Configuration.AppTransparency+Stub.swift in Sources */,\n",
        " \t\t\t\t6B610F6F29B95C8500EF848A /* HeroControllableMock.swift in Sources */,\n",
        " \t\t\t\t679F96CC2D4C0798000EA3AA /* ProductsPickerTrackerMock.swift in Sources */,\n",
        "+\t\t\t\tF3D7C5C52D5B60BD00D8AA5D /* ConfigurationTests.swift in Sources */,\n",
        " \t\t\t\t9A82F2AE2B69161A006E35A1 /* SignInModels+Stub.swift in Sources */,\n",
        " \t\t\t\tF3F99D192CB938FE0051AD9D /* TileOverrideImageSelectionStrategyHeroPresentationNoneTests.swift in Sources */,\n",
        " \t\t\t\t671AA0372CEB858D00FA61BC /* DirectBillingMapperTests.swift in Sources */,\n",
        "\"\"\"},\n",
        "], tokenize = False, add_generation_prompt = True)\n",
        "\n",
        "from vllm import SamplingParams\n",
        "sampling_params = SamplingParams(\n",
        "    temperature = 0.8,\n",
        "    top_p = 0.95,\n",
        "    max_tokens = 1024,\n",
        ")\n",
        "output = model.fast_generate(\n",
        "    [text],\n",
        "    sampling_params = sampling_params,\n",
        "    lora_request = None,\n",
        ")[0].outputs[0].text\n",
        "\n",
        "output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Colxz9TAVMsi"
      },
      "source": [
        "And now with the LoRA we just trained with GRPO - we first save the LoRA first!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AL-BcuB1VLIv"
      },
      "outputs": [],
      "source": [
        "model.save_lora(\"grpo_saved_lora\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CwpbwnDBVRLg"
      },
      "source": [
        "Now we load the LoRA and test:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zf_OY5WMVOxF"
      },
      "outputs": [],
      "source": [
        "text = tokenizer.apply_chat_template([\n",
        "    {\"role\" : \"system\", \"content\" : SYSTEM_PROMPT},\n",
        "    {\"role\" : \"user\", \"content\" : \"\"\"\\\n",
        "    diff --git forkSrcPrefix/ProjectTests/Tests/Configuration/ConfigurationTests.swift forkDstPrefix/ProjectTests/Tests/Configuration/ConfigurationTests.swift\n",
        "new file mode 100644\n",
        "index 0000000000000000000000000000000000000000..43875ea954cfda9cb9b28cded3c45473680c3382\n",
        "--- /dev/null\n",
        "+++ forkDstPrefix/ProjectTests/Tests/Configuration/ConfigurationTests.swift\n",
        "@@ -0,0 +1,45 @@\n",
        "+@testable import Project\n",
        "+import XCTest\n",
        "+\n",
        "+class ConfigurationTests: XCTestCase {\n",
        "+    private let decoder = JSONDecoder()\n",
        "+\n",
        "+    // MARK: - Tests\n",
        "+    func test_ConfigurationDecoding_WhenProfileCacheValuesWasProvided_UsesProvidedValues() throws {\n",
        "+        // Given\n",
        "+        let json = \"\"\"\n",
        "+        {\n",
        "+            \"version\": \"1.0.0\",\n",
        "+            \"profileWithoutPinDuration\": 3600,\n",
        "+            \"profileWithPinDuration\": 86400\n",
        "+        }\n",
        "+        \"\"\"\n",
        "+\n",
        "+        // When\n",
        "+        let configuration = try decoder.decode(Configuration.self, from: Data(json.utf8))\n",
        "+\n",
        "+        // Then\n",
        "+        XCTAssertEqual(configuration.version, \"1.0.0\")\n",
        "+        XCTAssertEqual(configuration.profileWithoutPinDuration, 3600)\n",
        "+        XCTAssertEqual(configuration.profileWithPinDuration, 86400)\n",
        "+    }\n",
        "+\n",
        "+    func test_ConfigurationDecoding_WhenProfileCacheValuesNotProvided_UsesDefaultValue() throws {\n",
        "+        // Given\n",
        "+        let json = \"\"\"\n",
        "+        {\n",
        "+            \"version\": \"1.1.0\"\n",
        "+        }\n",
        "+        \"\"\"\n",
        "+\n",
        "+        // When\n",
        "+        let configuration = try decoder.decode(Configuration.self, from: Data(json.utf8))\n",
        "+\n",
        "+        // Then\n",
        "+        XCTAssertEqual(configuration.version, \"1.1.0\")\n",
        "+        /// Default value is 30 minutes\n",
        "+        XCTAssertEqual(configuration.profileWithoutPinDuration, 1800)\n",
        "+        /// Default value is 12 hours\n",
        "+        XCTAssertEqual(configuration.profileWithPinDuration, 43200)\n",
        "+    }\n",
        "+}\n",
        "diff --git forkSrcPrefix/Project.xcodeproj/project.pbxproj forkDstPrefix/Project.xcodeproj/project.pbxproj\n",
        "index 61b65d35abf98eda4c0845782bb0f1b58d5c89e5..f28dc9c7fae76b2076d59f25043974a39b32c68d 100644\n",
        "--- forkSrcPrefix/Project.xcodeproj/project.pbxproj\n",
        "+++ forkDstPrefix/Project.xcodeproj/project.pbxproj\n",
        "@@ -2767,6 +2767,7 @@\n",
        " \t\tF3D3D8D32AB8A5F000891143 /* BrowseViewController+DisplayInvalidatedCache.swift in Sources */ = {isa = PBXBuildFile; fileRef = F3D3D8D22AB8A5F000891143 /* BrowseViewController+DisplayInvalidatedCache.swift */; };\n",
        " \t\tF3D3D8F32AB9F1BE00891143 /* InfoStorage+TokenValidityManager.swift in Sources */ = {isa = PBXBuildFile; fileRef = F3D3D8F22AB9F1BE00891143 /* InfoStorage+TokenValidityManager.swift */; };\n",
        " \t\tF3D3D8F82AB9F2C700891143 /* InfoStorageTokenExpirationManagerTests.swift in Sources */ = {isa = PBXBuildFile; fileRef = F3D3D8F72AB9F2C700891143 /* InfoStorageTokenExpirationManagerTests.swift */; };\n",
        "+\t\tF3D7C5C52D5B60BD00D8AA5D /* ConfigurationTests.swift in Sources */ = {isa = PBXBuildFile; fileRef = F3D7C5C42D5B60BD00D8AA5D /* ConfigurationTests.swift */; };\n",
        " \t\tF3DD7E322BA86CF8003B7301 /* Dependency+DispatchQueueServiceName.swift in Sources */ = {isa = PBXBuildFile; fileRef = F3DD7E312BA86CF8003B7301 /* Dependency+DispatchQueueServiceName.swift */; };\n",
        " \t\tF3DFC0192C93387900FA7E2E /* EPGAssetModel.swift in Sources */ = {isa = PBXBuildFile; fileRef = F3DFC0182C93387900FA7E2E /* EPGAssetModel.swift */; };\n",
        " \t\tF3E04CDB2C063C480054E143 /* Browse.InteractiveSchedule+ViewController+DataDisplaying.swift in Sources */ = {isa = PBXBuildFile; fileRef = F3E04CDA2C063C480054E143 /* Browse.InteractiveSchedule+ViewController+DataDisplaying.swift */; };\n",
        "@@ -6495,6 +6496,7 @@\n",
        " \t\tF3D3D8D22AB8A5F000891143 /* BrowseViewController+DisplayInvalidatedCache.swift */ = {isa = PBXFileReference; lastKnownFileType = sourcecode.swift; path = \"BrowseViewController+DisplayInvalidatedCache.swift\"; sourceTree = \"<group>\"; };\n",
        " \t\tF3D3D8F22AB9F1BE00891143 /* InfoStorage+TokenValidityManager.swift */ = {isa = PBXFileReference; lastKnownFileType = sourcecode.swift; path = \"InfoStorage+TokenValidityManager.swift\"; sourceTree = \"<group>\"; };\n",
        " \t\tF3D3D8F72AB9F2C700891143 /* InfoStorageTokenExpirationManagerTests.swift */ = {isa = PBXFileReference; lastKnownFileType = sourcecode.swift; path = InfoStorageTokenExpirationManagerTests.swift; sourceTree = \"<group>\"; };\n",
        "+\t\tF3D7C5C42D5B60BD00D8AA5D /* ConfigurationTests.swift */ = {isa = PBXFileReference; lastKnownFileType = sourcecode.swift; path = ConfigurationTests.swift; sourceTree = \"<group>\"; };\n",
        " \t\tF3DD7E312BA86CF8003B7301 /* Dependency+DispatchQueueServiceName.swift */ = {isa = PBXFileReference; lastKnownFileType = sourcecode.swift; path = \"Dependency+DispatchQueueServiceName.swift\"; sourceTree = \"<group>\"; };\n",
        " \t\tF3DFC0182C93387900FA7E2E /* EPGAssetModel.swift */ = {isa = PBXFileReference; lastKnownFileType = sourcecode.swift; path = EPGAssetModel.swift; sourceTree = \"<group>\"; };\n",
        " \t\tF3E04CDA2C063C480054E143 /* Browse.InteractiveSchedule+ViewController+DataDisplaying.swift */ = {isa = PBXFileReference; lastKnownFileType = sourcecode.swift; path = \"Browse.InteractiveSchedule+ViewController+DataDisplaying.swift\"; sourceTree = \"<group>\"; };\n",
        "@@ -15192,6 +15194,7 @@\n",
        " \t\tC412C10929F162D5003C2D36 /* Configuration */ = {\n",
        " \t\t\tisa = PBXGroup;\n",
        " \t\t\tchildren = (\n",
        "+\t\t\t\tF3D7C5C42D5B60BD00D8AA5D /* ConfigurationTests.swift */,\n",
        " \t\t\t\tC412C10A29F162DC003C2D36 /* ConfigurationVideoQualitySettingsTests.swift */,\n",
        " \t\t\t);\n",
        " \t\t\tpath = Configuration;\n",
        "@@ -22326,6 +22329,7 @@\n",
        " \t\t\t\tA5E557262637264C00ED2159 /* Configuration.AppTransparency+Stub.swift in Sources */,\n",
        " \t\t\t\t6B610F6F29B95C8500EF848A /* HeroControllableMock.swift in Sources */,\n",
        " \t\t\t\t679F96CC2D4C0798000EA3AA /* ProductsPickerTrackerMock.swift in Sources */,\n",
        "+\t\t\t\tF3D7C5C52D5B60BD00D8AA5D /* ConfigurationTests.swift in Sources */,\n",
        " \t\t\t\t9A82F2AE2B69161A006E35A1 /* SignInModels+Stub.swift in Sources */,\n",
        " \t\t\t\tF3F99D192CB938FE0051AD9D /* TileOverrideImageSelectionStrategyHeroPresentationNoneTests.swift in Sources */,\n",
        " \t\t\t\t671AA0372CEB858D00FA61BC /* DirectBillingMapperTests.swift in Sources */,\n",
        "\"\"\"},\n",
        "], tokenize = False, add_generation_prompt = True)\n",
        "\n",
        "from vllm import SamplingParams\n",
        "sampling_params = SamplingParams(\n",
        "    temperature = 0.8,\n",
        "    top_p = 0.95,\n",
        "    max_tokens = 1024,\n",
        ")\n",
        "output = model.fast_generate(\n",
        "    text,\n",
        "    sampling_params = sampling_params,\n",
        "    lora_request = model.load_lora(\"grpo_saved_lora\"),\n",
        ")[0].outputs[0].text\n",
        "\n",
        "output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6aDgFfhFYIAS"
      },
      "source": [
        "Our reasoning model is much better - it's not always correct, since we only trained it for an hour or so - it'll be better if we extend the sequence length and train for longer!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-NUEmHFSYNTp"
      },
      "source": [
        "<a name=\"Save\"></a>\n",
        "### Saving to float16 for VLLM\n",
        "\n",
        "We also support saving to `float16` directly. Select `merged_16bit` for float16 or `merged_4bit` for int4. We also allow `lora` adapters as a fallback. Use `push_to_hub_merged` to upload to your Hugging Face account! You can go to https://huggingface.co/settings/tokens for your personal tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NjXGTkp7YNtB"
      },
      "outputs": [],
      "source": [
        "# Merge to 16bit\n",
        "if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_16bit\",)\n",
        "if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"merged_16bit\", token = \"\")\n",
        "\n",
        "# Merge to 4bit\n",
        "if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_4bit\",)\n",
        "if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"merged_4bit\", token = \"\")\n",
        "\n",
        "# Just LoRA adapters\n",
        "if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"lora\",)\n",
        "if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"lora\", token = \"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52WMb3k_YPt8"
      },
      "source": [
        "### GGUF / llama.cpp Conversion\n",
        "To save to `GGUF` / `llama.cpp`, we support it natively now! We clone `llama.cpp` and we default save it to `q8_0`. We allow all methods like `q4_k_m`. Use `save_pretrained_gguf` for local saving and `push_to_hub_gguf` for uploading to HF.\n",
        "\n",
        "Some supported quant methods (full list on our [Wiki page](https://github.com/unslothai/unsloth/wiki#gguf-quantization-options)):\n",
        "* `q8_0` - Fast conversion. High resource use, but generally acceptable.\n",
        "* `q4_k_m` - Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q4_K.\n",
        "* `q5_k_m` - Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q5_K.\n",
        "\n",
        "[**NEW**] To finetune and auto export to Ollama, try our [Ollama notebook](https://colab.research.google.com/drive/1WZDi7APtQ9VsvOrQSSC5DDtxq159j8iZ?usp=sharing)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QyEjW-WuYQIm"
      },
      "outputs": [],
      "source": [
        "# Save to 8bit Q8_0\n",
        "if False: model.save_pretrained_gguf(\"model\", tokenizer,)\n",
        "# Remember to go to https://huggingface.co/settings/tokens for a token!\n",
        "# And change hf to your username!\n",
        "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, token = \"\")\n",
        "\n",
        "# Save to 16bit GGUF\n",
        "if False: model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"f16\")\n",
        "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"f16\", token = \"\")\n",
        "\n",
        "# Save to q4_k_m GGUF\n",
        "if False: model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"q4_k_m\")\n",
        "if True: model.push_to_hub_gguf(\"Tavernari/git-commit-message\", tokenizer, quantization_method = \"q4_k_m\", token = \"\")\n",
        "\n",
        "# Save to multiple GGUF options - much faster if you want multiple!\n",
        "if False:\n",
        "    model.push_to_hub_gguf(\n",
        "        \"hf/model\", # Change hf to your username!\n",
        "        tokenizer,\n",
        "        quantization_method = [\"q4_k_m\", \"q8_0\", \"q5_k_m\",],\n",
        "        token = \"\",\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zhy893V-m8Xt"
      },
      "source": [
        "Now, use the `model-unsloth.gguf` file or `model-unsloth-Q4_K_M.gguf` file in llama.cpp or a UI based system like Jan or Open WebUI. You can install Jan [here](https://github.com/janhq/jan) and Open WebUI [here](https://github.com/open-webui/open-webui)\n",
        "\n",
        "And we're done! If you have any questions on Unsloth, we have a [Discord](https://discord.gg/unsloth) channel! If you find any bugs or want to keep updated with the latest LLM stuff, or need help, join projects etc, feel free to join our Discord!\n",
        "\n",
        "Some other links:\n",
        "1. Llama 3.2 Conversational notebook. [Free Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_(1B_and_3B)-Conversational.ipynb)\n",
        "2. Saving finetunes to Ollama. [Free notebook](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3_(8B)-Ollama.ipynb)\n",
        "3. Llama 3.2 Vision finetuning - Radiography use case. [Free Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_(11B)-Vision.ipynb)\n",
        "6. See notebooks for DPO, ORPO, Continued pretraining, conversational finetuning and more on our [documentation](https://docs.unsloth.ai/get-started/unsloth-notebooks)!\n",
        "\n",
        "<div class=\"align-center\">\n",
        "  <a href=\"https://unsloth.ai\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n",
        "  <a href=\"https://discord.gg/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord.png\" width=\"145\"></a>\n",
        "  <a href=\"https://docs.unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/blob/main/images/documentation%20green%20button.png?raw=true\" width=\"125\"></a>\n",
        "\n",
        "  Join Discord if you need help + ⭐️ <i>Star us on <a href=\"https://github.com/unslothai/unsloth\">Github</a> </i> ⭐️\n",
        "</div>\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}