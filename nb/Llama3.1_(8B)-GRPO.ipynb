{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7JMZx6zdiYS9"
      },
      "source": [
        "To run this, press \"*Runtime*\" and press \"*Run all*\" on a **free** Tesla T4 Google Colab instance!\n",
        "<div class=\"align-center\">\n",
        "<a href=\"https://unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n",
        "<a href=\"https://discord.gg/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord button.png\" width=\"145\"></a>\n",
        "<a href=\"https://docs.unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/blob/main/images/documentation%20green%20button.png?raw=true\" width=\"125\"></a></a> Join Discord if you need help + ‚≠ê <i>Star us on <a href=\"https://github.com/unslothai/unsloth\">Github</a> </i> ‚≠ê\n",
        "</div>\n",
        "\n",
        "To install Unsloth on your own computer, follow the installation instructions on our Github page [here](https://docs.unsloth.ai/get-started/installing-+-updating).\n",
        "\n",
        "You will learn how to do [data prep](#Data), how to [train](#Train), how to [run the model](#Inference), & [how to save it](#Save)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k4Dv_1zsiYS_"
      },
      "source": [
        "### News"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jba-3HHkiYS_"
      },
      "source": [
        "**Read our [blog post](https://unsloth.ai/blog/r1-reasoning) for guidance on how to train reasoning models.**\n",
        "\n",
        "Visit our docs for all our [model uploads](https://docs.unsloth.ai/get-started/all-our-models) and [notebooks](https://docs.unsloth.ai/get-started/unsloth-notebooks).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QSWGPx67iYS_"
      },
      "source": [
        "### Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "C47myxY3iYTA"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# Skip restarting message in Colab\n",
        "import sys; modules = list(sys.modules.keys())\n",
        "for x in modules: sys.modules.pop(x) if \"PIL\" in x or \"google\" in x else None\n",
        "\n",
        "!pip install unsloth vllm\n",
        "!pip install --upgrade pillow\n",
        "# If you are running this notebook on local, you need to install `diffusers` too\n",
        "# !pip install diffusers\n",
        "# Temporarily install a specific TRL nightly version\n",
        "!pip install git+https://github.com/huggingface/trl.git@e95f9fb74a3c3647b86f251b7e230ec51c64b72b"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O46ZhN-FiYTB"
      },
      "source": [
        "### Unsloth"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P1zyu9Ug2XEt"
      },
      "source": [
        "Use `PatchFastRL` before all functions to patch GRPO and other RL algorithms!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "59DIs5BMcvjN",
        "outputId": "8ba30f74-fccf-422a-e340-b53d0e496737"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unsloth: Patching Xformers to fix some performance issues.\n",
            "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "ü¶• Unsloth Zoo will now patch everything to make training faster!\n",
            "INFO 02-11 19:46:51 __init__.py:190] Automatically detected platform cuda.\n"
          ]
        }
      ],
      "source": [
        "from unsloth import FastLanguageModel, PatchFastRL\n",
        "PatchFastRL(\"GRPO\", FastLanguageModel)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R8-SLRUB2gwM"
      },
      "source": [
        "Load up `Llama 3.1 8B Instruct`, and set parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 577,
          "referenced_widgets": [
            "a566121bee624e2c9f1da0f83fda0cc8",
            "f52cdf6f43ac41a19c4c2fcfe5dbfdcd",
            "914513eff0d04423a08a9f073fff54ae",
            "923bd4dbd5b24bb680b31137aace0a90",
            "ce685c709e05484d9cce2644b02cb9a6",
            "61792e7d6dbd4dc7bb08310b82353254",
            "f4af6f1d3921466f935a5352225b7ce6",
            "d99c6133e926494aaa87dea98f942300",
            "b5aea9666f89456a8631f28c109f2df5",
            "fc23e14490b641c7a3c9569963d3a55d",
            "ae32779d25154b189d109204575aea20",
            "4a5f6616cb554d7488ca56cc0822a8ef",
            "23125bba97a54a259fef0a5c2a21c5b7",
            "c1189e57fac240d198bf32a9d53724c1",
            "34c76a6fcc6748e4a729be2a72bac37c",
            "96b160908ede4a988d4c1dee4916f435",
            "cd7523e7dedd421598766c367265b7f0",
            "06334364ece842b8bd3d96d8e7ac5948",
            "ad4488bce4614ab4807cb845e4b3e473",
            "72d5da490a874914a99d7f6815ca0651",
            "277619b8caa84de9be5168f54111a6cb",
            "bfd11a6c23e84bd1a819f351cb0c08e7"
          ]
        },
        "id": "DkIvEkIIkEyB",
        "outputId": "b16810ad-e20a-4299-a5b1-d87e1d92f31d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==((====))==  Unsloth 2025.2.5: Fast Llama patching. Transformers: 4.48.2.\n",
            "   \\\\   /|    GPU: NVIDIA A100-SXM4-40GB. Max memory: 39.557 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.5.1+cu124. CUDA: 8.0. CUDA Toolkit: 12.4. Triton: 3.1.0\n",
            "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post3. FA2 = False]\n",
            " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
            "Unsloth: Your GPU cannot handle sequence lengths of 32000 due to limited GPU memory.\n",
            "Unsloth: Your GPU can only handle approximately the maximum sequence length of 32000.\n",
            "Unsloth: vLLM loading unsloth/meta-llama-3.1-8b-instruct-bnb-4bit with actual GPU utilization = 17.65%\n",
            "Unsloth: Your GPU has CUDA compute capability 8.0 with VRAM = 39.56 GB.\n",
            "Unsloth: Using conservativeness = 1.0. Chunked prefill tokens = 6144. Num Sequences = 128.\n",
            "Unsloth: vLLM's KV Cache can use up to 0.81 GB. Also swap space = 6 GB.\n",
            "INFO 02-11 19:53:57 config.py:542] This model supports multiple tasks: {'embed', 'reward', 'classify', 'generate', 'score'}. Defaulting to 'generate'.\n",
            "Unsloth: vLLM Bitsandbytes config using kwargs = {'load_in_8bit': False, 'load_in_4bit': True, 'bnb_4bit_compute_dtype': 'bfloat16', 'bnb_4bit_quant_storage': 'uint8', 'bnb_4bit_quant_type': 'nf4', 'bnb_4bit_use_double_quant': True, 'llm_int8_enable_fp32_cpu_offload': False, 'llm_int8_has_fp16_weight': False, 'llm_int8_skip_modules': ['lm_head', 'multi_modal_projector', 'merger', 'modality_projection'], 'llm_int8_threshold': 6.0}\n",
            "INFO 02-11 19:53:57 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.2) with config: model='unsloth/meta-llama-3.1-8b-instruct-bnb-4bit', speculative_config=None, tokenizer='unsloth/meta-llama-3.1-8b-instruct-bnb-4bit', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=6144, download_dir=None, load_format=LoadFormat.BITSANDBYTES, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=bitsandbytes, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=unsloth/meta-llama-3.1-8b-instruct-bnb-4bit, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"level\":0,\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":128}, use_cached_outputs=False, \n",
            "INFO 02-11 19:53:59 model_runner.py:1110] Starting to load model unsloth/meta-llama-3.1-8b-instruct-bnb-4bit...\n",
            "INFO 02-11 19:53:59 loader.py:1102] Loading weights with BitsAndBytes quantization.  May take a while ...\n",
            "INFO 02-11 19:53:59 weight_utils.py:252] Using model weights format ['*.safetensors']\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a566121bee624e2c9f1da0f83fda0cc8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4a5f6616cb554d7488ca56cc0822a8ef"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO 02-11 19:54:03 model_runner.py:1115] Loading model weights took 5.3229 GB\n",
            "INFO 02-11 19:54:05 worker.py:267] Memory profiling takes 1.21 seconds\n",
            "INFO 02-11 19:54:05 worker.py:267] the current vLLM instance can use total_gpu_memory (39.56GiB) x gpu_memory_utilization (0.18) = 6.98GiB\n",
            "INFO 02-11 19:54:05 worker.py:267] model weights take 5.32GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 0.74GiB; the rest of the memory reserved for KV Cache is 0.92GiB.\n",
            "INFO 02-11 19:54:06 executor_base.py:110] # CUDA blocks: 468, # CPU blocks: 3072\n",
            "INFO 02-11 19:54:06 executor_base.py:115] Maximum concurrency for 6144 tokens per request: 1.22x\n",
            "INFO 02-11 19:54:10 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Capturing CUDA graph shapes: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 19/19 [00:28<00:00,  1.50s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO 02-11 19:54:39 model_runner.py:1562] Graph capturing finished in 29 secs, took 0.52 GiB\n",
            "INFO 02-11 19:54:39 llm_engine.py:431] init engine (profile, create kv cache, warmup model) took 35.09 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "from unsloth import is_bfloat16_supported\n",
        "import torch\n",
        "max_seq_length = 32000 # Can increase for longer reasoning traces\n",
        "lora_rank = 32 # Larger rank = smarter, but slower\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"meta-llama/meta-Llama-3.1-8B-Instruct\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    load_in_4bit = True, # False for LoRA 16bit\n",
        "    fast_inference = True, # Enable vLLM fast inference\n",
        "    max_lora_rank = lora_rank,\n",
        "    gpu_memory_utilization = 0.6, # Reduce if out of memory\n",
        ")\n",
        "\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = lora_rank, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
        "    target_modules = [\n",
        "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
        "    ], # Remove QKVO if out of memory\n",
        "    lora_alpha = lora_rank,\n",
        "    use_gradient_checkpointing = \"unsloth\", # Enable long context finetuning\n",
        "    random_state = 3407,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7KGgPgk_5S8r"
      },
      "source": [
        "### Data Prep\n",
        "<a name=\"Data\"></a>\n",
        "\n",
        "We directly leverage [@willccbb](https://gist.github.com/willccbb/4676755236bb08cab5f4e54a0475d6fb) for data prep and all reward functions. You are free to create your own!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "cXk993X6C2ZZ",
        "outputId": "57deec11-53c0-4180-b923-293cb7e4b03b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "4d6c30fdbd5b4ea3baa7de4086fc9b22",
            "d383262dcc0f48aab32bd26e6d485a2e",
            "8f8aca0f3324407b83031d73baeb8ff1",
            "bf861db5ecc24b55b2746912f4dfc291",
            "128d9f7d0c2043dcb4fac23a19f49f01",
            "c756ef3a913f4084b3f8653fd6a4b43a",
            "5e63dea0690f41d6990541c36d5a76ba",
            "882e86a70db14817b141c682a34b9e4d",
            "7a06be90afcb4b4ba680618445abd297",
            "a3de39b578f5491e95808ae53fc0dc92",
            "98f24de2172f475e955078506fe2c582"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/2535 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4d6c30fdbd5b4ea3baa7de4086fc9b22"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "import re\n",
        "from datasets import load_dataset, Dataset\n",
        "\n",
        "# Load and prep dataset\n",
        "SYSTEM_PROMPT = \"\"\"\n",
        "Respond in the following format:\n",
        "<reasoning>\n",
        "...\n",
        "</reasoning>\n",
        "<answer>\n",
        "...\n",
        "</answer>\n",
        "\n",
        "Expected <answer> content format:\n",
        "```git-commit-message\n",
        "The git commit title (max 60-character)\n",
        "\n",
        "The git commit description (Only readable text)\n",
        "```\n",
        "\n",
        "You must avoid prefix on title.\n",
        "You are an expert developer, so you know how to read all kinds of code syntax.\n",
        "Read the git patch diff calmly from top to bottom, paying attention to each addition, deletion, and unchanged line carefully.\n",
        "Focus on changes, not only the last or first lines, and figure out the main idea of the input.\n",
        "If complex, break it down into smaller parts to organize your thoughts.\n",
        "If JSON or declaration structures are present, pay attention to the special case on JSONs diff to avoid misinterpretation, but if it's a regular code, focus on the context and the changes made. Write a commit message based on the git diff provided.\n",
        "Read the diff and write a commit message that accurately describes the changes made.\n",
        "Focus in write why instead of what on your description message.\n",
        "Your message is to add a kind of explanation for future consulting, so be clear and concise.\n",
        "\"\"\"\n",
        "\n",
        "XML_COT_FORMAT = \"\"\"\\\n",
        "<reasoning>\n",
        "{reasoning}\n",
        "</reasoning>\n",
        "<answer>\n",
        "{answer}\n",
        "</answer>\n",
        "\"\"\"\n",
        "\n",
        "def extract_xml_answer(text: str) -> str:\n",
        "    answer = text.split(\"<answer>\")[-1]\n",
        "    answer = answer.split(\"</answer>\")[0]\n",
        "    return answer.strip()\n",
        "\n",
        "# uncomment middle messages for 1-shot prompting\n",
        "def get_gsm8k_questions(split = \"train\") -> Dataset:\n",
        "    data = load_dataset('Tavernari/git-commit-message', 'default')[split] # type: ignore\n",
        "    data = data.map(lambda x: { # type: ignore\n",
        "        'prompt': [\n",
        "            {'role': 'system', 'content': SYSTEM_PROMPT},\n",
        "            {'role': 'user', 'content': x['input']}\n",
        "        ],\n",
        "        'answer': x['output']\n",
        "    }) # type: ignore\n",
        "    return data # type: ignore\n",
        "\n",
        "dataset = get_gsm8k_questions()\n",
        "\n",
        "# Reward functions\n",
        "def correctness_reward_func(prompts, completions, answer, **kwargs) -> list[float]:\n",
        "    responses = [completion[0]['content'] for completion in completions]\n",
        "    q = prompts[0][-1]['content']\n",
        "    extracted_responses = [extract_xml_answer(r) for r in responses]\n",
        "    print('-'*20, f\"Question:\\n{q}\", f\"\\nAnswer:\\n{answer[0]}\", f\"\\nResponse:\\n{responses[0]}\", f\"\\nExtracted:\\n{extracted_responses[0]}\")\n",
        "    return [2.0 if r == a else 0.0 for r, a in zip(extracted_responses, answer)]\n",
        "\n",
        "def int_reward_func(completions, **kwargs) -> list[float]:\n",
        "    responses = [completion[0]['content'] for completion in completions]\n",
        "    extracted_responses = [extract_xml_answer(r) for r in responses]\n",
        "    return [0.5 if r.isdigit() else 0.0 for r in extracted_responses]\n",
        "\n",
        "def strict_format_reward_func(completions, **kwargs) -> list[float]:\n",
        "    \"\"\"Reward function that checks if the completion has a specific format.\"\"\"\n",
        "    pattern = r\"^<reasoning>\\n.*?\\n</reasoning>\\n<answer>\\n.*?\\n</answer>\\n$\"\n",
        "    responses = [completion[0][\"content\"] for completion in completions]\n",
        "    matches = [re.match(pattern, r) for r in responses]\n",
        "    return [0.5 if match else 0.0 for match in matches]\n",
        "\n",
        "def soft_format_reward_func(completions, **kwargs) -> list[float]:\n",
        "    \"\"\"Reward function that checks if the completion has a specific format.\"\"\"\n",
        "    pattern = r\"<reasoning>.*?</reasoning>\\s*<answer>.*?</answer>\"\n",
        "    responses = [completion[0][\"content\"] for completion in completions]\n",
        "    matches = [re.match(pattern, r) for r in responses]\n",
        "    return [0.5 if match else 0.0 for match in matches]\n",
        "\n",
        "def count_xml(text) -> float:\n",
        "    count = 0.0\n",
        "    if text.count(\"<reasoning>\\n\") == 1:\n",
        "        count += 0.125\n",
        "    if text.count(\"\\n</reasoning>\\n\") == 1:\n",
        "        count += 0.125\n",
        "    if text.count(\"\\n<answer>\\n\") == 1:\n",
        "        count += 0.125\n",
        "        count -= len(text.split(\"\\n</answer>\\n\")[-1])*0.001\n",
        "    if text.count(\"\\n</answer>\") == 1:\n",
        "        count += 0.125\n",
        "        count -= (len(text.split(\"\\n</answer>\")[-1]) - 1)*0.001\n",
        "    return count\n",
        "\n",
        "def xmlcount_reward_func(completions, **kwargs) -> list[float]:\n",
        "    contents = [completion[0][\"content\"] for completion in completions]\n",
        "    return [count_xml(c) for c in contents]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ux6iqP7z5YOo"
      },
      "source": [
        "<a name=\"Train\"></a>\n",
        "### Train the model\n",
        "\n",
        "Now set up GRPO Trainer and all configurations!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ptqkXK2D4d6p",
        "outputId": "c0b27ef4-2368-45ac-bd85-0216f401e10b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "torch.distributed process group is initialized, but parallel_mode != ParallelMode.DISTRIBUTED. In order to use Torch DDP, launch your script with `python -m torch.distributed.launch\n"
          ]
        }
      ],
      "source": [
        "from trl import GRPOConfig, GRPOTrainer\n",
        "training_args = GRPOConfig(\n",
        "    use_vllm = True, # use vLLM for fast inference!\n",
        "    learning_rate = 5e-6,\n",
        "    adam_beta1 = 0.9,\n",
        "    adam_beta2 = 0.99,\n",
        "    weight_decay = 0.1,\n",
        "    warmup_ratio = 0.1,\n",
        "    lr_scheduler_type = \"cosine\",\n",
        "    optim = \"paged_adamw_8bit\",\n",
        "    logging_steps = 1,\n",
        "    bf16 = is_bfloat16_supported(),\n",
        "    fp16 = not is_bfloat16_supported(),\n",
        "    per_device_train_batch_size = 1,\n",
        "    gradient_accumulation_steps = 1, # Increase to 4 for smoother training\n",
        "    num_generations = 6, # Decrease if out of memory\n",
        "    max_prompt_length = 256,\n",
        "    max_completion_length = 200,\n",
        "    # num_train_epochs = 1, # Set to 1 for a full training run\n",
        "    max_steps = 250,\n",
        "    save_steps = 250,\n",
        "    max_grad_norm = 0.1,\n",
        "    report_to = \"none\", # Can use Weights & Biases\n",
        "    output_dir = \"outputs\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r9Mv8UZO5hz-"
      },
      "source": [
        "And let's run the trainer! If you scroll up, you'll see a table of rewards. The goal is to see the `reward` column increase!\n",
        "\n",
        "You might have to wait 150 to 200 steps for any action. You'll probably get 0 reward for the first 100 steps. Please be patient!\n",
        "\n",
        "| Step | Training Loss | reward    | reward_std | completion_length | kl       |\n",
        "|------|---------------|-----------|------------|-------------------|----------|\n",
        "| 1    | 0.000000      | 0.125000  | 0.000000   | 200.000000        | 0.000000 |\n",
        "| 2    | 0.000000      | 0.072375  | 0.248112   | 200.000000        | 0.000000 |\n",
        "| 3    | 0.000000      | -0.079000 | 0.163776   | 182.500000        | 0.000005 |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "vzOuSVCL_GA9",
        "outputId": "8fd24d1a-10e6-4c19-9e7d-7822c9e7e91a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
            "   \\\\   /|    Num examples = 2,535 | Num Epochs = 1\n",
            "O^O/ \\_/ \\    Batch size per device = 1 | Gradient Accumulation steps = 1\n",
            "\\        /    Total batch size = 1 | Total steps = 250\n",
            " \"-____-\"     Number of trainable parameters = 83,886,080\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------------- Question:\n",
            "diff --git a/src/main/kotlin/math/TwoSum.kt b/src/main/kotlin/math/TwoSum.kt\n",
            "new file mode 100644\n",
            "index 0000000..7b18f4d\n",
            "--- /dev/null\n",
            "+++ b/src/main/kotlin/math/TwoSum.kt\n",
            "@@ -0,0 +1,19 @@\n",
            "+package math\n",
            "+/**\n",
            "+ * Try all the pairs in the array and see if any of them add up to the target number.\n",
            "+ * @param nums Array of integers.\n",
            "+ * @param target Integer target.\n",
            "+ * @return Indices of the two numbers such that they add up to target.\n",
            "+ */\n",
            "+fun twoSum(nums: IntArray, target: Int): IntArray{\n",
            "+    for (index1 in nums.indices) {\n",
            "+        val startIndex = index1 + 1\n",
            "+        for (index2 in startIndex..nums.lastIndex) {\n",
            "+            if (nums[index1] + nums[index2] == target) {\n",
            "+                return intArrayOf(index1, index2)\n",
            "+            }\n",
            "+        }\n",
            "+    }\n",
            "+    return intArrayOf(0,1)\n",
            "+\n",
            "+}\n",
            "\\ No newline at end of file \n",
            "Answer:\n",
            "```git-commit-message\n",
            "Add TwoSum function for finding indices of two numbers that sum to target\n",
            "\n",
            "This commit introduces the `twoSum` function, which takes an array of integers and a target integer as input. It systematically checks all pairs of numbers in the array to determine if any two add up to the specified target. This function is essential for solving problems related to finding pairs in an array that fulfill a given criteria, enhancing the utility of the math module for future development and algorithm enhancements.\n",
            "``` \n",
            "Response:\n",
            "<reasoning>\n",
            "This change is likely to improve the clarity and conciseness of the code by removing the explicit `guard let` statement and using optional chaining to handle the case where the optional is nil. \n",
            "</reasoning>\n",
            "\n",
            "<answer>\n",
            "The diff is changing the implementation of two extensions in the `Optional` protocol: `isNilOrEmpty` and `nonEmpty`. The original implementation uses a `guard let` statement to unwrap the optional and then checks its isEmpty property. The new implementation removes the guard statement and uses optional chaining with the `??` operator to provide a default value of `true` if the optional is nil. \n",
            "Extracted:\n",
            "The diff is changing the implementation of two extensions in the `Optional` protocol: `isNilOrEmpty` and `nonEmpty`. The original implementation uses a `guard let` statement to unwrap the optional and then checks its isEmpty property. The new implementation removes the guard statement and uses optional chaining with the `??` operator to provide a default value of `true` if the optional is nil.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='8' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [  8/250 01:00 < 40:28, 0.10 it/s, Epoch 0.00/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>reward</th>\n",
              "      <th>reward_std</th>\n",
              "      <th>completion_length</th>\n",
              "      <th>kl</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.229000</td>\n",
              "      <td>0.297446</td>\n",
              "      <td>176.666672</td>\n",
              "      <td>0.000815</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>75.333336</td>\n",
              "      <td>0.000999</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>133.166672</td>\n",
              "      <td>0.000885</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>183.833344</td>\n",
              "      <td>0.000749</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>113.500000</td>\n",
              "      <td>0.000890</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>128.333344</td>\n",
              "      <td>0.000613</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------------- Question:\n",
            "diff --git a/src/merge.py b/src/merge.py\n",
            "--- a/src/merge.py\n",
            "+++ b/src/merge.py\n",
            "@@ -13,5 +13,5 @@\n",
            "         text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)\n",
            "         text = re.sub(r'\\s+', ' ', text)\n",
            "-        return text.strip().lower()\n",
            "+        return text.strip()\n",
            "     return text \n",
            "Answer:\n",
            "```git-commit-message\n",
            "Update text processing to preserve case in merge.py\n",
            "\n",
            "This change modifies the text processing function to return the stripped text without converting it to lower case. Preserving the original case may be important for subsequent processing or user expectations, allowing for better handling of cases where case sensitivity is significant.\n",
            "``` \n",
            "Response:\n",
            "git-commit-message\n",
            "Remove unnecessary method chaining and potential data loss\n",
            "\n",
            "In this commit, the author removed the `lower()` method from the `text` processing pipeline, which may lead to unexpected behavior or data loss when dealing with non-ASCII texts. While the intention seems to replace non-ASCII characters with spaces, a more robust approach could be to handle these characters based on the project's needs, or to log a warning to inform the developer of potential issues when dealing with non-ASCII texts. \n",
            "Extracted:\n",
            "git-commit-message\n",
            "Remove unnecessary method chaining and potential data loss\n",
            "\n",
            "In this commit, the author removed the `lower()` method from the `text` processing pipeline, which may lead to unexpected behavior or data loss when dealing with non-ASCII texts. While the intention seems to replace non-ASCII characters with spaces, a more robust approach could be to handle these characters based on the project's needs, or to log a warning to inform the developer of potential issues when dealing with non-ASCII texts.\n",
            "-------------------- Question:\n",
            "diff --git a/Gemfile.lock b/Gemfile.lock\n",
            "index fb44e1e35..657f863c1 100644\n",
            "--- a/Gemfile.lock\n",
            "+++ b/Gemfile.lock\n",
            "@@ -3,7 +3,7 @@ GEM\n",
            "   specs:\n",
            "     CFPropertyList (3.0.5)\n",
            "       rexml\n",
            "-    activesupport (6.1.7.1)\n",
            "+    activesupport (6.1.7.3)\n",
            "       concurrent-ruby (~> 1.0, >= 1.0.2)\n",
            "       i18n (>= 1.6, < 2)\n",
            "       minitest (>= 5.1)\n",
            "@@ -79,7 +79,7 @@ GEM\n",
            "     colored2 (3.1.2)\n",
            "     commander (4.6.0)\n",
            "       highline (~> 2.0.0)\n",
            "-    concurrent-ruby (1.1.10)\n",
            "+    concurrent-ruby (1.2.2)\n",
            "     cork (0.3.0)\n",
            "       colored2 (~> 3.1)\n",
            "     danger (9.0.0)\n",
            "@@ -246,7 +246,7 @@ GEM\n",
            "     memoist (0.16.2)\n",
            "     mini_magick (4.11.0)\n",
            "     mini_mime (1.1.2)\n",
            "-    minitest (5.17.0)\n",
            "+    minitest (5.18.0)\n",
            "     molinillo (0.8.0)\n",
            "     multi_json (1.15.0)\n",
            "     multipart-post (2.0.0)\n",
            "@@ -298,7 +298,7 @@ GEM\n",
            "       tty-cursor (~> 0.7)\n",
            "     typhoeus (1.4.0)\n",
            "       ethon (>= 0.9.0)\n",
            "-    tzinfo (2.0.5)\n",
            "+    tzinfo (2.0.6)\n",
            "       concurrent-ruby (~> 1.0)\n",
            "     uber (0.1.0)\n",
            "     unf (0.1.4)\n",
            "@@ -321,7 +321,7 @@ GEM\n",
            "     xcpretty-travis-formatter (1.0.1)\n",
            "       xcpretty (~> 0.2, >= 0.0.7)\n",
            "     xcresult (0.2.1)\n",
            "-    zeitwerk (2.6.6)\n",
            "+    zeitwerk (2.6.7)\n",
            " \n",
            " PLATFORMS\n",
            "   arm64-darwin-22 \n",
            "Answer:\n",
            "```git-commit-message\n",
            "Update gem dependencies in Gemfile.lock\n",
            "\n",
            "This commit updates several gem dependencies to their newer versions in the Gemfile.lock. The updates include activesupport (from 6.1.7.1 to 6.1.7.3), concurrent-ruby (from 1.1.10 to 1.2.2), minitest (from 5.17.0 to 5.18.0), tzinfo (from 2.0.5 to 2.0.6), and zeitwerk (from 2.6.6 to 2.6.7). These changes are essential for maintaining compatibility, fixing bugs, and ensuring that the project benefits from the latest features and performance improvements provided by the updated gems.\n",
            "``` \n",
            "Response:\n",
            "git-commit-message\n",
            "Bump activesupport, concurrent-ruby and tzinfo\n",
            "\n",
            "The commit updates several gem dependencies to their latest versions:<ul>\n",
            "   <li>activesupport is updated to 6.1.7.3</li>\n",
            "   <li>concurrent-ruby is updated to 1.2.2</li>\n",
            "   <li>tzinfo is updated to 2.0.6</li>\n",
            "</ul>\n",
            "These updates likely improve the dependencies' functionality, fix bugs, or provide security patches. \n",
            "Extracted:\n",
            "git-commit-message\n",
            "Bump activesupport, concurrent-ruby and tzinfo\n",
            "\n",
            "The commit updates several gem dependencies to their latest versions:<ul>\n",
            "   <li>activesupport is updated to 6.1.7.3</li>\n",
            "   <li>concurrent-ruby is updated to 1.2.2</li>\n",
            "   <li>tzinfo is updated to 2.0.6</li>\n",
            "</ul>\n",
            "These updates likely improve the dependencies' functionality, fix bugs, or provide security patches.\n",
            "-------------------- Question:\n",
            "diff --git a/graph/bellman_ford.ts b/graph/bellman_ford.ts\n",
            "new file mode 100644\n",
            "index 00000000..70e326d2\n",
            "--- /dev/null\n",
            "+++ b/graph/bellman_ford.ts\n",
            "@@ -0,0 +1,45 @@\n",
            "+/**\n",
            "+ * @function bellmanFord\n",
            "+ * @description Compute the shortest path from a source node to all other nodes. If there is negative weight cycle, returns undefined. The input graph is in adjacency list form. It is a multidimensional array of edges. graph[i] holds the edges for the i'th node. Each edge is a 2-tuple where the 0'th item is the destination node, and the 1'th item is the edge weight.\n",
            "+ * @Complexity_Analysis\n",
            "+ * Time complexity: O(E*V)\n",
            "+ * Space Complexity: O(V)\n",
            "+ * @param {[number, number][][]} graph - The graph in adjacency list form\n",
            "+ * @param {number} start - The source node\n",
            "+ * @return {number[] | undefined} - The shortest path to each node, undefined if there is negative weight cycle\n",
            "+ * @see https://en.wikipedia.org/wiki/Bellman%E2%80%93Ford_algorithm\n",
            "+ */\n",
            "+export const bellmanFord = (graph: [number, number][][], start: number): number[] | undefined => {\n",
            "+  // We save the shortest distance to each node in `distances`. If a node is\n",
            "+  // unreachable from the start node, its distance is Infinity.\n",
            "+  let distances = Array(graph.length).fill(Infinity);\n",
            "+  distances[start] = 0;\n",
            "+\n",
            "+  // On the i'th iteration, we compute all shortest paths that consists of i+1\n",
            "+  // nodes. If we compute this V-1 times, we will have computed all simple\n",
            "+  // shortest paths in the graph because a shortest path has at most V nodes.\n",
            "+  for (let i = 0; i < graph.length - 1; ++i) {\n",
            "+    for (let node = 0; node < graph.length; ++node) {\n",
            "+      for (const [child, weight] of graph[node]) {\n",
            "+        const new_distance = distances[node] + weight;\n",
            "+        if (new_distance < distances[child]) {\n",
            "+          distances[child] = new_distance\n",
            "+        }\n",
            "+      }\n",
            "+    }\n",
            "+  }\n",
            "+\n",
            "+  // Look through all edges. If the shortest path to a destination node d is\n",
            "+  // larger than the distance to source node s and weight(s->d), then the path\n",
            "+  // to s must have a negative weight cycle.\n",
            "+  for (let node = 0; node < graph.length; ++node) {\n",
            "+    for (const [child, weight] of graph[node]) {\n",
            "+      if (distances[child] > distances[node] + weight) {\n",
            "+        return undefined;\n",
            "+      }\n",
            "+    }\n",
            "+  }\n",
            "+\n",
            "+  return distances;\n",
            "+}\n",
            "+\n",
            "diff --git a/graph/test/bellman_ford.test.ts b/graph/test/bellman_ford.test.ts\n",
            "new file mode 100644\n",
            "index 00000000..77928a63\n",
            "--- /dev/null\n",
            "+++ b/graph/test/bellman_ford.test.ts\n",
            "@@ -0,0 +1,88 @@\n",
            "+import { bellmanFord } from \"../bellman_ford\";\n",
            "+\n",
            "+const init_graph = (N: number): [number, number][][] => {\n",
            "+  let graph = Array(N);\n",
            "+  for (let i = 0; i < N; ++i) {\n",
            "+    graph[i] = [];\n",
            "+  }\n",
            "+  return graph;\n",
            "+}\n",
            "+\n",
            "+describe(\"bellmanFord\", () => {\n",
            "+\n",
            "+  const add_edge = (graph: [number, number][][], a: number, b: number, weight: number) => {\n",
            "+    graph[a].push([b, weight]);\n",
            "+    graph[b].push([a, weight]);\n",
            "+  }\n",
            "+\n",
            "+  it(\"should return the correct value\", () => {\n",
            "+    let graph = init_graph(9);\n",
            "+    add_edge(graph, 0, 1, 4);\n",
            "+    add_edge(graph, 0, 7, 8);\n",
            "+    add_edge(graph, 1, 2, 8);\n",
            "+    add_edge(graph, 1, 7, 11);\n",
            "+    add_edge(graph, 2, 3, 7);\n",
            "+    add_edge(graph, 2, 5, 4);\n",
            "+    add_edge(graph, 2, 8, 2);\n",
            "+    add_edge(graph, 3, 4, 9);\n",
            "+    add_edge(graph, 3, 5, 14);\n",
            "+    add_edge(graph, 4, 5, 10);\n",
            "+    add_edge(graph, 5, 6, 2);\n",
            "+    add_edge(graph, 6, 7, 1);\n",
            "+    add_edge(graph, 6, 8, 6);\n",
            "+    add_edge(graph, 7, 8, 7);\n",
            "+    expect(bellmanFord(graph, 0)).toStrictEqual([0, 4, 12, 19, 21, 11, 9, 8, 14]);\n",
            "+  });\n",
            "+\n",
            "+  it(\"should return the correct value for single element graph\", () => {\n",
            "+    expect(bellmanFord([[]], 0)).toStrictEqual([0]);\n",
            "+  });\n",
            "+\n",
            "+  let linear_graph = init_graph(4);\n",
            "+  add_edge(linear_graph, 0, 1, 1);\n",
            "+  add_edge(linear_graph, 1, 2, 2);\n",
            "+  add_edge(linear_graph, 2, 3, 3);\n",
            "+  test.each([[0, [0, 1, 3, 6]], [1, [1, 0, 2, 5]], [2, [3, 2, 0, 3]], [3, [6, 5, 3, 0]]])(\n",
            "+    \"correct result for linear graph with source node %i\",\n",
            "+    (source, result) => {\n",
            "+      expect(bellmanFord(linear_graph, source)).toStrictEqual(result);\n",
            "+    }\n",
            "+  );\n",
            "+\n",
            "+  let unreachable_graph = init_graph(3);\n",
            "+  add_edge(unreachable_graph, 0, 1, 1);\n",
            "+  test.each([[0, [0, 1, Infinity]], [1, [1, 0, Infinity]], [2, [Infinity, Infinity, 0]]])(\n",
            "+    \"correct result for graph with unreachable nodes with source node %i\",\n",
            "+    (source, result) => {\n",
            "+      expect(bellmanFord(unreachable_graph, source)).toStrictEqual(result);\n",
            "+    }\n",
            "+  );\n",
            "+})\n",
            "+\n",
            "+describe(\"bellmanFord negative cycle graphs\", () => {\n",
            "+  it(\"should returned undefined for 2-node graph with negative cycle\", () => {\n",
            "+    let basic = init_graph(2);\n",
            "+    basic[0].push([1, 2]);\n",
            "+    basic[1].push([0, -3]);\n",
            "+    expect(bellmanFord(basic, 0)).toStrictEqual(undefined);\n",
            "+    expect(bellmanFord(basic, 1)).toStrictEqual(undefined);\n",
            "+  });\n",
            "+\n",
            "+  it(\"should returned undefined for graph with negative cycle\", () => {\n",
            "+    let negative = init_graph(5);\n",
            "+    negative[0].push([1, 6]);\n",
            "+    negative[0].push([3, 7]);\n",
            "+    negative[1].push([2, 5]);\n",
            "+    negative[1].push([3, 8]);\n",
            "+    negative[1].push([4, -4]);\n",
            "+    negative[2].push([1, -4]);\n",
            "+    negative[3].push([2, -3]);\n",
            "+    negative[3].push([4, 9]);\n",
            "+    negative[4].push([0, 3]);\n",
            "+    negative[4].push([2, 7]);\n",
            "+    for (let i = 0; i < 5; ++i) {\n",
            "+      expect(bellmanFord(negative, i)).toStrictEqual(undefined);\n",
            "+    }\n",
            "+  });\n",
            "+});\n",
            "+ \n",
            "Answer:\n",
            "```git-commit-message\n",
            "Implement Bellman-Ford algorithm for shortest paths\n",
            "\n",
            "This commit introduces the Bellman-Ford algorithm to compute the shortest paths from a source node to all other nodes in a graph represented as an adjacency list. The implementation ensures that it detects negative weight cycles, returning undefined if one exists. The time and space complexities are documented, making it easier to understand performance implications. Additionally, comprehensive unit tests have been added to validate the functionality, covering various scenarios including unreachable nodes and graphs with negative cycles. This provides a robust foundation for shortest path calculations in future applications.\n",
            "``` \n",
            "Response:\n",
            "git-commit-message\n",
            "Refactor and add tests for Bellman-Ford algorithm implementation\n",
            "\n",
            "The provided code implements the Bellman-Ford algorithm for finding the shortest path from a source node to all other nodes in a weighted graph. The code is well-structured and follows a clear explanation of how the algorithm works. However, there are some minor improvements that can be made for better readability and maintainability.\n",
            "\n",
            "Firstly, the code uses a lot of complex variable names, making it hard to understand the intent behind them. For example, `new_distance` can be renamed to `new_distance_to_child`.\n",
            "\n",
            "Secondly, there are comments that are not necessary and can be removed for a cleaner code.\n",
            "\n",
            "Lastly, the edge weight is not validated, but since it's a weighted graph, it should be checked for edge cases like negative weights. This is already handled by the Bellman-Ford algorithm itself as a negative cycle is returned in such cases.\n",
            "\n",
            "The tests cover various scenarios including a single-element graph, \n",
            "Extracted:\n",
            "git-commit-message\n",
            "Refactor and add tests for Bellman-Ford algorithm implementation\n",
            "\n",
            "The provided code implements the Bellman-Ford algorithm for finding the shortest path from a source node to all other nodes in a weighted graph. The code is well-structured and follows a clear explanation of how the algorithm works. However, there are some minor improvements that can be made for better readability and maintainability.\n",
            "\n",
            "Firstly, the code uses a lot of complex variable names, making it hard to understand the intent behind them. For example, `new_distance` can be renamed to `new_distance_to_child`.\n",
            "\n",
            "Secondly, there are comments that are not necessary and can be removed for a cleaner code.\n",
            "\n",
            "Lastly, the edge weight is not validated, but since it's a weighted graph, it should be checked for edge cases like negative weights. This is already handled by the Bellman-Ford algorithm itself as a negative cycle is returned in such cases.\n",
            "\n",
            "The tests cover various scenarios including a single-element graph,\n",
            "-------------------- Question:\n",
            "diff --git a/src/main/kotlin/dynamic_programming/PalindromePartitioning.kt b/src/main/kotlin/dynamic_programming/PalindromePartitioning.kt\n",
            "index 9d4c688..b4dd1f1 100644\n",
            "--- a/src/main/kotlin/dynamic_programming/PalindromePartitioning.kt\n",
            "+++ b/src/main/kotlin/dynamic_programming/PalindromePartitioning.kt\n",
            "@@ -53,9 +53,9 @@ fun palindromePartition(string: String, i: Int, j: Int): Int {\n",
            "     }\n",
            "     dp[i][j] = mn\n",
            "     return dp[i][j]\n",
            "-\n",
            " }\n",
            " \n",
            "+\n",
            " /**\n",
            "  * memoization table\n",
            "  **/\n",
            "@@ -66,11 +66,6 @@ lateinit var dp: Array<Array<Int>>\n",
            "  * @param String the string on which algorithm is to be operated\n",
            "  */\n",
            " fun initialize(string: String): Int {\n",
            "-\n",
            "     dp = Array(string.length) { Array(string.length) { -1 } }\n",
            "     return palindromePartition(string, 0, string.length - 1)\n",
            " }\n",
            "-\n",
            "-\n",
            "-\n",
            "- \n",
            "Answer:\n",
            "```git-commit-message\n",
            "Refactor PalindromePartitioning.kt for improved readability\n",
            "\n",
            "The changes made to the PalindromePartitioning.kt file feature structural modifications aimed at enhancing code readability and maintainability. Unnecessary line breaks were removed to streamline the presentation of functions and comments. The goal of these adjustments is to provide a clearer understanding of the code flow and organization, ensuring that developers can more easily navigate and comprehend the palindromic partitioning logic implemented.\n",
            "``` \n",
            "Response:\n",
            "```3a1b2c3\n",
            "Added memoization table for PalindromePartitioning algorithm\n",
            "\n",
            "The commit improved performance of the PalindromePartitioning algorithm in Kotlin by initializing a memoization table to store the results of subproblems, reducing redundant calculations and increasing efficiency when handling large inputs.\n",
            "```\n",
            "\n",
            "In this commit message:\n",
            "\n",
            "- The first line is the commit title, which is short (less than 60 characters), and summarizes the main change made.\n",
            "- The description explains the purpose and impact of the change, focusing on why it was made (performance improvement) rather than what was changed (the addition of a memoization table). \n",
            "Extracted:\n",
            "```3a1b2c3\n",
            "Added memoization table for PalindromePartitioning algorithm\n",
            "\n",
            "The commit improved performance of the PalindromePartitioning algorithm in Kotlin by initializing a memoization table to store the results of subproblems, reducing redundant calculations and increasing efficiency when handling large inputs.\n",
            "```\n",
            "\n",
            "In this commit message:\n",
            "\n",
            "- The first line is the commit title, which is short (less than 60 characters), and summarizes the main change made.\n",
            "- The description explains the purpose and impact of the change, focusing on why it was made (performance improvement) rather than what was changed (the addition of a memoization table).\n",
            "-------------------- Question:\n",
            "diff --git a/poetry.lock b/poetry.lock\n",
            "index d7f4eeb..71e19df 100644\n",
            "--- a/poetry.lock\n",
            "+++ b/poetry.lock\n",
            "@@ -455,13 +455,13 @@ testing = [\"fields\", \"hunter\", \"process-tests\", \"pytest-xdist\", \"virtualenv\"]\n",
            " \n",
            " [[package]]\n",
            " name = \"pytest-httpserver\"\n",
            "-version = \"1.0.10\"\n",
            "+version = \"1.0.12\"\n",
            " description = \"pytest-httpserver is a httpserver for pytest\"\n",
            " optional = false\n",
            " python-versions = \">=3.8\"\n",
            " files = [\n",
            "-    {file = \"pytest_httpserver-1.0.10-py3-none-any.whl\", hash = \"sha256:d40e0cc3d61ed6e4d80f52a796926d557a7db62b17e43b3e258a78a3c34becb9\"},\n",
            "-    {file = \"pytest_httpserver-1.0.10.tar.gz\", hash = \"sha256:77b9fbc2eb0a129cfbbacc8fe57e8cafe071d506489f31fe31e62f1b332d9905\"},\n",
            "+    {file = \"pytest_httpserver-1.0.12-py3-none-any.whl\", hash = \"sha256:dae1c79ec7aeda83bfaaf4d0a400867a4b1bc6bf668244daaf13aa814e3022da\"},\n",
            "+    {file = \"pytest_httpserver-1.0.12.tar.gz\", hash = \"sha256:c14600b8efb9ea8d7e63251a242ab987f13028b36d3d397ffaca3c929f67eb16\"},\n",
            " ]\n",
            " \n",
            " [package.dependencies] \n",
            "Answer:\n",
            "```git-commit-message\n",
            "Update pytest-httpserver to version 1.0.12 in poetry.lock\n",
            "\n",
            "The update was made to ensure compatibility with recent features and bug fixes in pytest-httpserver. By upgrading to version 1.0.12, we benefit from improved functionality and security enhancements that come with the latest release. This change ensures our testing environment remains reliable and effective with the latest library versions.\n",
            "``` \n",
            "Response:\n",
            "reasoning\n",
            "The provided git patch diff shows the changes made to the `poetry.lock` file. The diff highlights an update to the `pytest-httpserver` package. Previously, the version was set to \"1.0.10\", but now it has been updated to \"1.0.12\".\n",
            "\n",
            "Looking at the changes, it appears that the version of the `pytest-httpserver` package has been bumped from \"1.0.10\" to \"1.0.12\". The hashes for the `whl` and `tar.gz` files associated with the updated package have also changed.\n",
            "\n",
            "It seems that the change is likely due to an update to the package, and the `poetry.lock` file has been updated accordingly to reflect this change.\n",
            "\n",
            "answers\n",
            "git-commit-message\n",
            "Bumped pytest-httpserver to version 1.0.12\n",
            "\n",
            "The `poetry.lock` file was updated to reflect the new version of the `pytest-httpserver` package, from \" \n",
            "Extracted:\n",
            "reasoning\n",
            "The provided git patch diff shows the changes made to the `poetry.lock` file. The diff highlights an update to the `pytest-httpserver` package. Previously, the version was set to \"1.0.10\", but now it has been updated to \"1.0.12\".\n",
            "\n",
            "Looking at the changes, it appears that the version of the `pytest-httpserver` package has been bumped from \"1.0.10\" to \"1.0.12\". The hashes for the `whl` and `tar.gz` files associated with the updated package have also changed.\n",
            "\n",
            "It seems that the change is likely due to an update to the package, and the `poetry.lock` file has been updated accordingly to reflect this change.\n",
            "\n",
            "answers\n",
            "git-commit-message\n",
            "Bumped pytest-httpserver to version 1.0.12\n",
            "\n",
            "The `poetry.lock` file was updated to reflect the new version of the `pytest-httpserver` package, from \"\n",
            "-------------------- Question:\n",
            "diff --git a/README.md b/README.md\n",
            "index 6a9d63c..a5aa951 100644\n",
            "--- a/README.md\n",
            "+++ b/README.md\n",
            "@@ -1,2 +1,13 @@\n",
            "-# Swift\n",
            "-All Algorithms implemented in Swift\n",
            "+# The Algorithms - Swift\n",
            "+\n",
            "+### All algorithms implemented in Swift (for education)\n",
            "+\n",
            "+These implementations are for learning purposes. They may be less efficient than the implementations in the Swift standard library.\n",
            "+\n",
            "+## Community Channel\n",
            "+\n",
            "+We're on [Gitter](https://gitter.im/TheAlgorithms)!  Please join us.\n",
            "+\n",
            "+## List of Algorithms\n",
            "+\n",
            "+See our [directory](DIRECTORY.md). \n",
            "Answer:\n",
            "```git-commit-message\n",
            "Update README with enhanced project description and resources\n",
            "\n",
            "The README.md file was updated to provide a clearer project title and a more informative description of the Swift algorithms implementation. Added a note on the educational purpose of the code and included community engagement information with a link to the Gitter channel for collaboration. Additionally, a reference to the directory of algorithms was introduced to improve navigation and accessibility for users.\n",
            "``` \n",
            "Response:\n",
            "`git-commit-message\n",
            "Implement algorithm repository updates\n",
            "\n",
            "Update README to include information about implementations, community and available algorithms.`\n",
            " \n",
            "In the provided Git patch diff, the changes made to the README.md file were to:\n",
            "- Remove a one-line title \"Swift\" and replace it with a more detailed and informative title \"# The Algorithms - Swift\".\n",
            "- Add explanatory text to clarify that the algorithms implemented in Swift are for education and may not be as efficient as those in the Swift standard library.\n",
            "- Add information about the community channel, specifically a link to a Gitter channel.\n",
            "- Add a reference to a list of available algorithms, directing the reader to a separate file called DIRECTORY.md. \n",
            "Extracted:\n",
            "`git-commit-message\n",
            "Implement algorithm repository updates\n",
            "\n",
            "Update README to include information about implementations, community and available algorithms.`\n",
            " \n",
            "In the provided Git patch diff, the changes made to the README.md file were to:\n",
            "- Remove a one-line title \"Swift\" and replace it with a more detailed and informative title \"# The Algorithms - Swift\".\n",
            "- Add explanatory text to clarify that the algorithms implemented in Swift are for education and may not be as efficient as those in the Swift standard library.\n",
            "- Add information about the community channel, specifically a link to a Gitter channel.\n",
            "- Add a reference to a list of available algorithms, directing the reader to a separate file called DIRECTORY.md.\n"
          ]
        }
      ],
      "source": [
        "trainer = GRPOTrainer(\n",
        "    model = model,\n",
        "    processing_class = tokenizer,\n",
        "    reward_funcs = [\n",
        "        xmlcount_reward_func,\n",
        "        soft_format_reward_func,\n",
        "        strict_format_reward_func,\n",
        "        int_reward_func,\n",
        "        correctness_reward_func,\n",
        "    ],\n",
        "    args = training_args,\n",
        "    train_dataset = dataset,\n",
        ")\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tlaUdxC_VHpz"
      },
      "source": [
        "<a name=\"Inference\"></a>\n",
        "### Inference\n",
        "Now let's try the model we just trained! First, let's first try the model without any GRPO trained:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qtcz_lpbVC92"
      },
      "outputs": [],
      "source": [
        "text = tokenizer.apply_chat_template([\n",
        "    {\"role\" : \"user\", \"content\" : \"\"\"\\\n",
        "    diff --git a/src/merge.py b/src/merge.py\n",
        "--- a/src/merge.py\n",
        "+++ b/src/merge.py\n",
        "@@ -13,5 +13,5 @@\n",
        "         text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)\n",
        "         text = re.sub(r'\\s+', ' ', text)\n",
        "-        return text.strip().lower()\n",
        "+        return text.strip()\n",
        "     return text\n",
        "    \"\"\"},\n",
        "], tokenize = False, add_generation_prompt = True)\n",
        "\n",
        "from vllm import SamplingParams\n",
        "sampling_params = SamplingParams(\n",
        "    temperature = 0.8,\n",
        "    top_p = 0.95,\n",
        "    max_tokens = 1024,\n",
        ")\n",
        "output = model.fast_generate(\n",
        "    [text],\n",
        "    sampling_params = sampling_params,\n",
        "    lora_request = None,\n",
        ")[0].outputs[0].text\n",
        "\n",
        "output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Colxz9TAVMsi"
      },
      "source": [
        "And now with the LoRA we just trained with GRPO - we first save the LoRA first!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AL-BcuB1VLIv"
      },
      "outputs": [],
      "source": [
        "model.save_lora(\"grpo_saved_lora\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CwpbwnDBVRLg"
      },
      "source": [
        "Now we load the LoRA and test:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zf_OY5WMVOxF"
      },
      "outputs": [],
      "source": [
        "text = tokenizer.apply_chat_template([\n",
        "    {\"role\" : \"system\", \"content\" : SYSTEM_PROMPT},\n",
        "    {\"role\" : \"user\", \"content\" : \"\"\"\\\n",
        "    diff --git a/src/merge.py b/src/merge.py\n",
        "--- a/src/merge.py\n",
        "+++ b/src/merge.py\n",
        "@@ -13,5 +13,5 @@\n",
        "         text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)\n",
        "         text = re.sub(r'\\s+', ' ', text)\n",
        "-        return text.strip().lower()\n",
        "+        return text.strip()\n",
        "     return text\n",
        "    \"\"\"},\n",
        "], tokenize = False, add_generation_prompt = True)\n",
        "\n",
        "from vllm import SamplingParams\n",
        "sampling_params = SamplingParams(\n",
        "    temperature = 0.8,\n",
        "    top_p = 0.95,\n",
        "    max_tokens = 1024,\n",
        ")\n",
        "output = model.fast_generate(\n",
        "    text,\n",
        "    sampling_params = sampling_params,\n",
        "    lora_request = model.load_lora(\"grpo_saved_lora\"),\n",
        ")[0].outputs[0].text\n",
        "\n",
        "output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6aDgFfhFYIAS"
      },
      "source": [
        "Our reasoning model is much better - it's not always correct, since we only trained it for an hour or so - it'll be better if we extend the sequence length and train for longer!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-NUEmHFSYNTp"
      },
      "source": [
        "<a name=\"Save\"></a>\n",
        "### Saving to float16 for VLLM\n",
        "\n",
        "We also support saving to `float16` directly. Select `merged_16bit` for float16 or `merged_4bit` for int4. We also allow `lora` adapters as a fallback. Use `push_to_hub_merged` to upload to your Hugging Face account! You can go to https://huggingface.co/settings/tokens for your personal tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NjXGTkp7YNtB"
      },
      "outputs": [],
      "source": [
        "# Merge to 16bit\n",
        "if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_16bit\",)\n",
        "if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"merged_16bit\", token = \"\")\n",
        "\n",
        "# Merge to 4bit\n",
        "if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_4bit\",)\n",
        "if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"merged_4bit\", token = \"\")\n",
        "\n",
        "# Just LoRA adapters\n",
        "if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"lora\",)\n",
        "if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"lora\", token = \"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52WMb3k_YPt8"
      },
      "source": [
        "### GGUF / llama.cpp Conversion\n",
        "To save to `GGUF` / `llama.cpp`, we support it natively now! We clone `llama.cpp` and we default save it to `q8_0`. We allow all methods like `q4_k_m`. Use `save_pretrained_gguf` for local saving and `push_to_hub_gguf` for uploading to HF.\n",
        "\n",
        "Some supported quant methods (full list on our [Wiki page](https://github.com/unslothai/unsloth/wiki#gguf-quantization-options)):\n",
        "* `q8_0` - Fast conversion. High resource use, but generally acceptable.\n",
        "* `q4_k_m` - Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q4_K.\n",
        "* `q5_k_m` - Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q5_K.\n",
        "\n",
        "[**NEW**] To finetune and auto export to Ollama, try our [Ollama notebook](https://colab.research.google.com/drive/1WZDi7APtQ9VsvOrQSSC5DDtxq159j8iZ?usp=sharing)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QyEjW-WuYQIm"
      },
      "outputs": [],
      "source": [
        "# Save to 8bit Q8_0\n",
        "if False: model.save_pretrained_gguf(\"model\", tokenizer,)\n",
        "# Remember to go to https://huggingface.co/settings/tokens for a token!\n",
        "# And change hf to your username!\n",
        "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, token = \"\")\n",
        "\n",
        "# Save to 16bit GGUF\n",
        "if False: model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"f16\")\n",
        "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"f16\", token = \"\")\n",
        "\n",
        "# Save to q4_k_m GGUF\n",
        "if False: model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"q4_k_m\")\n",
        "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"q4_k_m\", token = \"\")\n",
        "\n",
        "# Save to multiple GGUF options - much faster if you want multiple!\n",
        "if False:\n",
        "    model.push_to_hub_gguf(\n",
        "        \"hf/model\", # Change hf to your username!\n",
        "        tokenizer,\n",
        "        quantization_method = [\"q4_k_m\", \"q8_0\", \"q5_k_m\",],\n",
        "        token = \"\",\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wekVwi5WiYTE"
      },
      "source": [
        "Now, use the `model-unsloth.gguf` file or `model-unsloth-Q4_K_M.gguf` file in llama.cpp or a UI based system like Jan or Open WebUI. You can install Jan [here](https://github.com/janhq/jan) and Open WebUI [here](https://github.com/open-webui/open-webui)\n",
        "\n",
        "And we're done! If you have any questions on Unsloth, we have a [Discord](https://discord.gg/unsloth) channel! If you find any bugs or want to keep updated with the latest LLM stuff, or need help, join projects etc, feel free to join our Discord!\n",
        "\n",
        "Some other links:\n",
        "1. Llama 3.2 Conversational notebook. [Free Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_(1B_and_3B)-Conversational.ipynb)\n",
        "2. Saving finetunes to Ollama. [Free notebook](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3_(8B)-Ollama.ipynb)\n",
        "3. Llama 3.2 Vision finetuning - Radiography use case. [Free Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_(11B)-Vision.ipynb)\n",
        "6. See notebooks for DPO, ORPO, Continued pretraining, conversational finetuning and more on our [documentation](https://docs.unsloth.ai/get-started/unsloth-notebooks)!\n",
        "\n",
        "<div class=\"align-center\">\n",
        "  <a href=\"https://unsloth.ai\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n",
        "  <a href=\"https://discord.gg/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord.png\" width=\"145\"></a>\n",
        "  <a href=\"https://docs.unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/blob/main/images/documentation%20green%20button.png?raw=true\" width=\"125\"></a>\n",
        "\n",
        "  Join Discord if you need help + ‚≠êÔ∏è <i>Star us on <a href=\"https://github.com/unslothai/unsloth\">Github</a> </i> ‚≠êÔ∏è\n",
        "</div>\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "a566121bee624e2c9f1da0f83fda0cc8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f52cdf6f43ac41a19c4c2fcfe5dbfdcd",
              "IPY_MODEL_914513eff0d04423a08a9f073fff54ae",
              "IPY_MODEL_923bd4dbd5b24bb680b31137aace0a90"
            ],
            "layout": "IPY_MODEL_ce685c709e05484d9cce2644b02cb9a6"
          }
        },
        "f52cdf6f43ac41a19c4c2fcfe5dbfdcd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_61792e7d6dbd4dc7bb08310b82353254",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_f4af6f1d3921466f935a5352225b7ce6",
            "value": ""
          }
        },
        "914513eff0d04423a08a9f073fff54ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d99c6133e926494aaa87dea98f942300",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b5aea9666f89456a8631f28c109f2df5",
            "value": 1
          }
        },
        "923bd4dbd5b24bb680b31137aace0a90": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fc23e14490b641c7a3c9569963d3a55d",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_ae32779d25154b189d109204575aea20",
            "value": "Loading‚Äásafetensors‚Äácheckpoint‚Äáshards:‚Äá100%‚ÄáCompleted‚Äá|‚Äá1/1‚Äá[00:01&lt;00:00,‚Äá‚Äá1.85s/it]\n"
          }
        },
        "ce685c709e05484d9cce2644b02cb9a6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "61792e7d6dbd4dc7bb08310b82353254": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f4af6f1d3921466f935a5352225b7ce6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d99c6133e926494aaa87dea98f942300": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b5aea9666f89456a8631f28c109f2df5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fc23e14490b641c7a3c9569963d3a55d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ae32779d25154b189d109204575aea20": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4a5f6616cb554d7488ca56cc0822a8ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_23125bba97a54a259fef0a5c2a21c5b7",
              "IPY_MODEL_c1189e57fac240d198bf32a9d53724c1",
              "IPY_MODEL_34c76a6fcc6748e4a729be2a72bac37c"
            ],
            "layout": "IPY_MODEL_96b160908ede4a988d4c1dee4916f435"
          }
        },
        "23125bba97a54a259fef0a5c2a21c5b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cd7523e7dedd421598766c367265b7f0",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_06334364ece842b8bd3d96d8e7ac5948",
            "value": ""
          }
        },
        "c1189e57fac240d198bf32a9d53724c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ad4488bce4614ab4807cb845e4b3e473",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_72d5da490a874914a99d7f6815ca0651",
            "value": 1
          }
        },
        "34c76a6fcc6748e4a729be2a72bac37c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_277619b8caa84de9be5168f54111a6cb",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_bfd11a6c23e84bd1a819f351cb0c08e7",
            "value": "Loading‚Äásafetensors‚Äácheckpoint‚Äáshards:‚Äá100%‚ÄáCompleted‚Äá|‚Äá1/1‚Äá[00:01&lt;00:00,‚Äá‚Äá1.95s/it]\n"
          }
        },
        "96b160908ede4a988d4c1dee4916f435": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cd7523e7dedd421598766c367265b7f0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "06334364ece842b8bd3d96d8e7ac5948": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ad4488bce4614ab4807cb845e4b3e473": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "72d5da490a874914a99d7f6815ca0651": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "277619b8caa84de9be5168f54111a6cb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bfd11a6c23e84bd1a819f351cb0c08e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4d6c30fdbd5b4ea3baa7de4086fc9b22": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d383262dcc0f48aab32bd26e6d485a2e",
              "IPY_MODEL_8f8aca0f3324407b83031d73baeb8ff1",
              "IPY_MODEL_bf861db5ecc24b55b2746912f4dfc291"
            ],
            "layout": "IPY_MODEL_128d9f7d0c2043dcb4fac23a19f49f01"
          }
        },
        "d383262dcc0f48aab32bd26e6d485a2e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c756ef3a913f4084b3f8653fd6a4b43a",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_5e63dea0690f41d6990541c36d5a76ba",
            "value": "Map:‚Äá100%"
          }
        },
        "8f8aca0f3324407b83031d73baeb8ff1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_882e86a70db14817b141c682a34b9e4d",
            "max": 2535,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7a06be90afcb4b4ba680618445abd297",
            "value": 2535
          }
        },
        "bf861db5ecc24b55b2746912f4dfc291": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a3de39b578f5491e95808ae53fc0dc92",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_98f24de2172f475e955078506fe2c582",
            "value": "‚Äá2535/2535‚Äá[00:00&lt;00:00,‚Äá11243.88‚Äáexamples/s]"
          }
        },
        "128d9f7d0c2043dcb4fac23a19f49f01": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c756ef3a913f4084b3f8653fd6a4b43a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5e63dea0690f41d6990541c36d5a76ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "882e86a70db14817b141c682a34b9e4d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7a06be90afcb4b4ba680618445abd297": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a3de39b578f5491e95808ae53fc0dc92": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "98f24de2172f475e955078506fe2c582": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}